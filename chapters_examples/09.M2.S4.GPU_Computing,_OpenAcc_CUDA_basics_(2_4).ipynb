{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to GPU Computing (Colab version)\n",
        "\n",
        "In today's computational landscape, GPUs (Graphics Processing Units) have evolved beyond their original purpose of rendering graphics. They have become essential tools for high-performance computing (HPC), capable of performing a vast number of calculations simultaneously. This ability to handle massive parallelism makes GPUs particularly useful for scientific simulations, machine learning, and other computationally intensive tasks.\n",
        "\n",
        "In this section, we'll explore the basics of GPU computing, including the architecture of GPUs and the programming models that allow us to harness their power. Specifically, we'll focus on two main approaches: CUDA and OpenACC.\n",
        "\n",
        "## What is GPU Computing?\n",
        "\n",
        "GPU computing involves using a GPU to perform computations that would otherwise be handled by a CPU. Unlike CPUs, which have a few cores optimized for sequential processing, GPUs have thousands of smaller, more efficient cores designed for parallel tasks. This makes GPUs ideal for workloads that can be broken down into many smaller, independent tasks.\n",
        "\n",
        "## Why Use GPUs in HPC?\n",
        "\n",
        "- **Massive Parallelism**: GPUs can execute thousands of threads simultaneously, making them well-suited for parallel algorithms.\n",
        "- **High Throughput**: With their large number of cores, GPUs can process a significant amount of data in parallel, leading to faster execution times for many applications.\n",
        "- **Energy Efficiency**: GPUs can often perform more computations per watt compared to CPUs, making them a more energy-efficient choice for large-scale computations.\n",
        "\n",
        "In the following sections, we will dive into the basics of CUDA and OpenACC, two popular programming models for GPU computing.\n"
      ],
      "metadata": {
        "id": "MmDYSoc-vd_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Basics\n",
        "\n",
        "CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use NVIDIA GPUs for general-purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units).\n",
        "\n",
        "### Key Concepts in CUDA\n",
        "\n",
        "- **Thread**: The smallest unit of processing. Each thread runs a copy of the kernel (a function that runs on the GPU).\n",
        "- **Block**: A group of threads that can cooperate amongst themselves via shared memory. Blocks are organized into a grid.\n",
        "- **Grid**: A collection of blocks. The grid represents the entire execution space for a given kernel.\n",
        "\n",
        "Let's start by writing a simple CUDA program to perform matrix multiplication, a common operation in many scientific and engineering applications.\n"
      ],
      "metadata": {
        "id": "w8O4UnqjviCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Basics\n",
        "\n",
        "CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use NVIDIA GPUs for general-purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units).\n",
        "\n",
        "### Key Concepts in CUDA\n",
        "\n",
        "- **Thread**: The smallest unit of processing. Each thread runs a copy of the kernel (a function that runs on the GPU).\n",
        "- **Block**: A group of threads that can cooperate amongst themselves via shared memory. Blocks are organized into a grid.\n",
        "- **Grid**: A collection of blocks. The grid represents the entire execution space for a given kernel.\n",
        "\n",
        "Let's start by writing a simple CUDA program to perform matrix multiplication, a common operation in many scientific and engineering applications.\n"
      ],
      "metadata": {
        "id": "jNsCEt9Qvk7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the GPU available in Google Colab\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7WbZF-Zf9SK",
        "outputId": "f55dee76-80f4-4c07-db06-a0c7903bc046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  1 16:54:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to CUDA Programming\n",
        "\n",
        "In this example, you will learn how to write a basic CUDA program that adds two vectors. We will define the vector addition on the GPU, allocate memory on the GPU, and then copy the result back to the CPU.\n",
        "\n",
        "The steps are as follows:\n",
        "1. Define the vectors on the CPU.\n",
        "2. Copy the vectors from the CPU to the GPU.\n",
        "3. Perform the vector addition on the GPU.\n",
        "4. Copy the result back to the CPU.\n",
        "5. Print the result.\n",
        "\n",
        "Let's write, compile, and run a simple CUDA program for vector addition.\n"
      ],
      "metadata": {
        "id": "OJlwLeyWEsic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Writing the CUDA C code to a file for vector addition\n",
        "cuda_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA kernel for vector addition\n",
        "__global__ void vectorAdd(float *A, float *B, float *C, int n) {\n",
        "    int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    if (i < n) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1024;  // Vector size\n",
        "\n",
        "    // Allocate memory on the host (CPU)\n",
        "    size_t size = n * sizeof(float);\n",
        "    float *h_A = (float *)malloc(size);\n",
        "    float *h_B = (float *)malloc(size);\n",
        "    float *h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize vectors A and B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        h_A[i] = i * 1.0f;\n",
        "        h_B[i] = i * 2.0f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device (GPU)\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void **)&d_A, size);\n",
        "    cudaMalloc((void **)&d_B, size);\n",
        "    cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "    // Copy vectors from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the vector addition kernel\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);\n",
        "\n",
        "    // Copy the result vector from device to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the result (only showing the first 10 elements)\n",
        "    printf(\"Result of vector addition (first 10 elements):\\\\n\");\n",
        "    for (int i = 0; i < 10; ++i) {\n",
        "        printf(\"%f + %f = %f\\\\n\", h_A[i], h_B[i], h_C[i]);\n",
        "    }\n",
        "\n",
        "    // Free memory on device and host\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Save the CUDA code to a file\n",
        "with open('vector_addition.cu', 'w') as f:\n",
        "    f.write(cuda_code)\n",
        "\n",
        "# Step 3: Compile the CUDA program using nvcc\n",
        "!nvcc -o vector_addition vector_addition.cu\n",
        "\n",
        "# Step 4: Run the CUDA program\n",
        "!./vector_addition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w97ozX04E7NR",
        "outputId": "ad0e5f85-da6d-40e2-bcfe-f737424c0a0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of vector addition (first 10 elements):\n",
            "0.000000 + 0.000000 = 0.000000\n",
            "1.000000 + 2.000000 = 3.000000\n",
            "2.000000 + 4.000000 = 6.000000\n",
            "3.000000 + 6.000000 = 9.000000\n",
            "4.000000 + 8.000000 = 12.000000\n",
            "5.000000 + 10.000000 = 15.000000\n",
            "6.000000 + 12.000000 = 18.000000\n",
            "7.000000 + 14.000000 = 21.000000\n",
            "8.000000 + 16.000000 = 24.000000\n",
            "9.000000 + 18.000000 = 27.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Matrix Multiplication using CUDA\n",
        "\n",
        "In this section, we will explore how to perform matrix multiplication using CUDA. Matrix multiplication is a fundamental operation in many scientific and engineering applications. GPUs are highly suited for this task because they allow us to perform many operations in parallel, making the multiplication of large matrices significantly faster compared to running on the CPU.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Thread Hierarchy**: CUDA organizes computation into a grid of blocks, where each block contains multiple threads. Each thread operates on a small part of the data, and together they process the entire matrix.\n",
        "2. **Global Memory**: Data is stored in global memory on the GPU, which can be accessed by all threads. In this example, matrices `A`, `B`, and `C` are stored in global memory.\n",
        "3. **Kernel**: A CUDA kernel is a function that runs on the GPU, with each thread executing an instance of the kernel. In this example, the kernel `matrixMul` computes one element of the output matrix `C`.\n",
        "\n",
        "### Matrix Multiplication Overview:\n",
        "Matrix multiplication involves computing the dot product of rows of matrix `A` with columns of matrix `B` to produce an element in matrix `C`. Each thread calculates one element of the output matrix `C`.\n",
        "\n",
        "In this example, we:\n",
        "1. **Define a kernel (`matrixMul`)** that performs the matrix multiplication.\n",
        "2. **Allocate memory** on both the CPU (host) and the GPU (device).\n",
        "3. **Copy data** from the CPU to the GPU.\n",
        "4. **Launch the kernel** on the GPU, where each thread computes one element of the result matrix.\n",
        "5. **Copy the result** back to the CPU and display part of the result matrix.\n",
        "\n",
        "This example uses matrices of size 512x512, but CUDA allows us to scale this to much larger matrices efficiently.\n",
        "\n",
        "### Task:\n",
        "Run the CUDA code for matrix multiplication using 512x512 matrices and observe the result printed on the console (a part of the resulting matrix). This will help you understand how to perform parallel matrix computations on the GPU.\n"
      ],
      "metadata": {
        "id": "YQk9hEigFJ6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the CUDA C code for matrix multiplication to a file\n",
        "cuda_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 512  // Define the size of the matrix\n",
        "\n",
        "__global__ void matrixMul(float *A, float *B, float *C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0.0;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        for (int i = 0; i < n; ++i) {\n",
        "            sum += A[row * n + i] * B[i * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    float *h_A = (float *)malloc(size);\n",
        "    float *h_B = (float *)malloc(size);\n",
        "    float *h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize matrices A and B with more variation\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        int row = i / N;\n",
        "        int col = i % N;\n",
        "        h_A[i] = (row + 1) + (col % 5);  // Create variation across rows and columns\n",
        "        h_B[i] = (col + 1) * (row % 3 + 1);  // Create variation across rows and columns\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void **)&d_A, size);\n",
        "    cudaMalloc((void **)&d_B, size);\n",
        "    cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "    // Copy matrices from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define the block size and grid size\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Launch the matrix multiplication kernel\n",
        "    matrixMul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copy the result matrix back to the host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the result matrix\n",
        "    printf(\"Result matrix C (only showing a part of it):\\\\n\");\n",
        "    for (int i = 0; i < 10; ++i) {\n",
        "        for (int j = 0; j < 10; ++j) {\n",
        "            printf(\"%f \", h_C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Free memory on device and host\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open('matrix_mul.cu', 'w') as f:\n",
        "    f.write(cuda_code)\n"
      ],
      "metadata": {
        "id": "VAnfEQKOggER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the CUDA Matrix Multiplication Code\n",
        "\n",
        "This CUDA program performs matrix multiplication, a fundamental operation in many scientific and engineering applications, by leveraging the parallel processing power of GPUs.\n",
        "\n",
        "- **Matrix Size Definition**: `#define N 512` sets the matrix dimensions to 512x512, which can be adjusted as needed.\n",
        "\n",
        "- **CUDA Kernel Function (`matrixMul`)**: The kernel function performs the actual multiplication. Each thread computes a single element of the result matrix `C` by iterating over a row of matrix `A` and a column of matrix `B`. The `blockIdx` and `threadIdx` are used to determine the position of the element each thread is responsible for.\n",
        "\n",
        "- **Memory Management**:\n",
        "  - **Host Memory**: Memory for matrices `A`, `B`, and `C` is allocated on the host (CPU) using `malloc`.\n",
        "  - **Device Memory**: Corresponding memory on the GPU is allocated using `cudaMalloc`.\n",
        "  - **Data Transfer**: The matrices `A` and `B` are copied from the host to the GPU before computation, and the result matrix `C` is copied back from the GPU to the host after the kernel execution.\n",
        "\n",
        "- **Grid and Block Configuration**: The grid and block dimensions are set using `dim3`, with a 16x16 block size to distribute the workload among the GPU threads efficiently.\n",
        "\n",
        "- **Kernel Launch**: The `matrixMul` kernel is launched with the specified grid and block dimensions to perform the matrix multiplication on the GPU.\n",
        "\n",
        "- **Result Output**: After computation, a portion of the result matrix `C` is printed to verify the output.\n",
        "\n",
        "- **Memory Cleanup**: Finally, both host and device memory are freed to prevent memory leaks.\n",
        "\n",
        "This example highlights the essential steps in a CUDA program, including memory management, kernel configuration, and parallel computation, providing a foundation for more complex GPU-accelerated applications.\n"
      ],
      "metadata": {
        "id": "-HsHEWxbwQax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the CUDA Program\n",
        "\n",
        "Next, we'll compile and run the CUDA program to see how GPU-based matrix multiplication works. The result matrix should reflect the combined computation of the two input matrices.\n",
        "\n",
        "We'll use `nvcc`, the CUDA compiler, to compile the code and then execute the compiled binary. If everything is set up correctly, you should see the output of the matrix multiplication displayed below.\n"
      ],
      "metadata": {
        "id": "50agN1c2vrYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the CUDA program\n",
        "!nvcc -o matrix_mul matrix_mul.cu\n",
        "\n"
      ],
      "metadata": {
        "id": "e4kyWq0Vgh9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the CUDA program\n",
        "!./matrix_mul\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glr9Uv_cgih5",
        "outputId": "01a236bd-6dc5-4ccd-dd5d-a3ae058fdc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix C (only showing a part of it):\n",
            "3065.000000 6130.000000 9195.000000 12260.000000 15325.000000 18390.000000 21455.000000 24520.000000 27585.000000 30650.000000 \n",
            "4088.000000 8176.000000 12264.000000 16352.000000 20440.000000 24528.000000 28616.000000 32704.000000 36792.000000 40880.000000 \n",
            "5111.000000 10222.000000 15333.000000 20444.000000 25555.000000 30666.000000 35777.000000 40888.000000 45999.000000 51110.000000 \n",
            "6134.000000 12268.000000 18402.000000 24536.000000 30670.000000 36804.000000 42938.000000 49072.000000 55206.000000 61340.000000 \n",
            "7157.000000 14314.000000 21471.000000 28628.000000 35785.000000 42942.000000 50099.000000 57256.000000 64413.000000 71570.000000 \n",
            "8180.000000 16360.000000 24540.000000 32720.000000 40900.000000 49080.000000 57260.000000 65440.000000 73620.000000 81800.000000 \n",
            "9203.000000 18406.000000 27609.000000 36812.000000 46015.000000 55218.000000 64421.000000 73624.000000 82827.000000 92030.000000 \n",
            "10226.000000 20452.000000 30678.000000 40904.000000 51130.000000 61356.000000 71582.000000 81808.000000 92034.000000 102260.000000 \n",
            "11249.000000 22498.000000 33747.000000 44996.000000 56245.000000 67494.000000 78743.000000 89992.000000 101241.000000 112490.000000 \n",
            "12272.000000 24544.000000 36816.000000 49088.000000 61360.000000 73632.000000 85904.000000 98176.000000 110448.000000 122720.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the CUDA C code for dense and sparse matrix multiplication to a file\n",
        "cuda_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 512  // Define the size of the matrix\n",
        "\n",
        "// Kernel for dense matrix multiplication\n",
        "__global__ void denseMatrixMul(float *A, float *B, float *C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0.0;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        for (int i = 0; i < n; ++i) {\n",
        "            sum += A[row * n + i] * B[i * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for sparse matrix multiplication (simplified for demo purposes)\n",
        "__global__ void sparseMatrixMul(int *rowPtr, int *colInd, float *values, float *B, float *C, int n, int nnz) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0.0;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        for (int idx = rowPtr[row]; idx < rowPtr[row + 1]; ++idx) {\n",
        "            int j = colInd[idx];\n",
        "            sum += values[idx] * B[j * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "\n",
        "    // Allocate memory on the host for dense matrices\n",
        "    float *h_A = (float *)malloc(size);\n",
        "    float *h_B = (float *)malloc(size);\n",
        "    float *h_C_dense = (float *)malloc(size);\n",
        "    float *h_C_sparse = (float *)malloc(size);\n",
        "\n",
        "\n",
        "    // Initialize dense matrices A and B with varying values\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        h_A[i] = ((i % N) + 1) * ((i / N) % 3 + 1);  // Example: values are products of row and column indices with some variation\n",
        "        h_B[i] = ((i / N) + 1) * 0.5f * ((i % N) % 4 + 1);  // Example: values depend on both row and column indices with multiplication factors\n",
        "    }\n",
        "\n",
        "\n",
        "    // Allocate memory on the device for dense matrices\n",
        "    float *d_A, *d_B, *d_C_dense;\n",
        "    cudaMalloc((void **)&d_A, size);\n",
        "    cudaMalloc((void **)&d_B, size);\n",
        "    cudaMalloc((void **)&d_C_dense, size);\n",
        "\n",
        "    // Copy matrices from host to device for dense multiplication\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define the block size and grid size for dense multiplication\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Launch the dense matrix multiplication kernel\n",
        "    denseMatrixMul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C_dense, N);\n",
        "\n",
        "    // Copy the result matrix back to the host for dense multiplication\n",
        "    cudaMemcpy(h_C_dense, d_C_dense, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results for dense matrix multiplication\n",
        "    printf(\"Result matrix C (Dense - showing a part of it):\\\\n\");\n",
        "    for (int i = 0; i < 10; ++i) {\n",
        "        for (int j = 0; j < 10; ++j) {\n",
        "            printf(\"%f \", h_C_dense[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Example of sparse matrix multiplication\n",
        "    // Note: Sparse matrix in CSR format is represented by rowPtr, colInd, and values arrays\n",
        "    int nnz = 3 * N;  // Example: 3 non-zero entries per row\n",
        "    int *h_rowPtr = (int *)malloc((N + 1) * sizeof(int));\n",
        "    int *h_colInd = (int *)malloc(nnz * sizeof(int));\n",
        "    float *h_values = (float *)malloc(nnz * sizeof(float));\n",
        "\n",
        "\n",
        "    // Initialize sparse matrix A in CSR format\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_rowPtr[i] = i * 3;  // 3 non-zeros per row\n",
        "        for (int j = 0; j < 3; ++j) {\n",
        "            h_colInd[i * 3 + j] = (i + j) % N;  // Example column indices\n",
        "            h_values[i * 3 + j] = 1.0f;  // Non-zero values\n",
        "        }\n",
        "    }\n",
        "    h_rowPtr[N] = nnz;  // Last entry of rowPtr\n",
        "\n",
        "    // Allocate memory on the device for sparse matrix multiplication\n",
        "    int *d_rowPtr, *d_colInd;\n",
        "    float *d_values, *d_C_sparse;\n",
        "    cudaMalloc((void **)&d_rowPtr, (N + 1) * sizeof(int));\n",
        "    cudaMalloc((void **)&d_colInd, nnz * sizeof(int));\n",
        "    cudaMalloc((void **)&d_values, nnz * sizeof(float));\n",
        "    cudaMalloc((void **)&d_C_sparse, size);\n",
        "\n",
        "    // Copy sparse matrix data from host to device\n",
        "    cudaMemcpy(d_rowPtr, h_rowPtr, (N + 1) * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_colInd, h_colInd, nnz * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_values, h_values, nnz * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);  // Reuse B from dense multiplication\n",
        "\n",
        "    // Launch the sparse matrix multiplication kernel\n",
        "    sparseMatrixMul<<<blocksPerGrid, threadsPerBlock>>>(d_rowPtr, d_colInd, d_values, d_B, d_C_sparse, N, nnz);\n",
        "\n",
        "    // Copy the result matrix back to the host for sparse multiplication\n",
        "    cudaMemcpy(h_C_sparse, d_C_sparse, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results for sparse matrix multiplication\n",
        "    printf(\"\\\\nResult matrix C (Sparse - showing a part of it):\\\\n\");\n",
        "    for (int i = 0; i < 10; ++i) {\n",
        "        for (int j = 0; j < 10; ++j) {\n",
        "            printf(\"%f \", h_C_sparse[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Free memory on device and host\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C_dense);\n",
        "    cudaFree(d_rowPtr);\n",
        "    cudaFree(d_colInd);\n",
        "    cudaFree(d_values);\n",
        "    cudaFree(d_C_sparse);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C_dense);\n",
        "    free(h_C_sparse);\n",
        "    free(h_rowPtr);\n",
        "    free(h_colInd);\n",
        "    free(h_values);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open('matrix_mul.cu', 'w') as f:\n",
        "    f.write(cuda_code)\n"
      ],
      "metadata": {
        "id": "BAB8gBMShm4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the CUDA Code for Dense and Sparse Matrix Multiplication\n",
        "\n",
        "This CUDA program demonstrates two different approaches to matrix multiplication: dense and sparse. Matrix multiplication is a core operation in many high-performance computing applications, and understanding both dense and sparse implementations is crucial for optimizing performance depending on the data characteristics.\n",
        "\n",
        "### Dense Matrix Multiplication\n",
        "- **Kernel Function (`denseMatrixMul`)**: This kernel computes the product of two dense matrices `A` and `B`, storing the result in matrix `C`. Each thread is responsible for calculating a single element in the result matrix `C` by iterating over a row in `A` and a column in `B`.\n",
        "- **Host and Device Memory Management**:\n",
        "  - **Host Memory**: Allocated for matrices `A`, `B`, and `C_dense` using `malloc`.\n",
        "  - **Device Memory**: Corresponding memory is allocated on the GPU using `cudaMalloc`.\n",
        "  - **Data Transfer**: Matrices `A` and `B` are copied from host to device memory before the kernel is executed, and the resulting matrix `C_dense` is copied back to the host after the kernel execution.\n",
        "- **Kernel Launch Configuration**: The matrix multiplication kernel is launched with a grid and block configuration determined by the size of the matrix and the number of threads per block, maximizing GPU resource utilization.\n",
        "\n",
        "### Sparse Matrix Multiplication\n",
        "- **Sparse Matrix Representation**: Sparse matrices store only non-zero elements to save memory and computation time. Here, the sparse matrix `A` is represented in Compressed Sparse Row (CSR) format using three arrays: `rowPtr`, `colInd`, and `values`.\n",
        "  - **`rowPtr`**: Indicates the start and end of each row in the `colInd` and `values` arrays.\n",
        "  - **`colInd`**: Stores the column indices of the non-zero elements.\n",
        "  - **`values`**: Stores the actual non-zero values of the matrix.\n",
        "- **Kernel Function (`sparseMatrixMul`)**: This kernel performs matrix multiplication using the sparse matrix `A` and dense matrix `B`, storing the result in matrix `C_sparse`. Each thread computes a single element in `C_sparse` by accessing only the non-zero elements in `A`.\n",
        "- **Host and Device Memory Management**:\n",
        "  - **Host Memory**: Additional memory is allocated for the sparse matrix representation (`rowPtr`, `colInd`, and `values`).\n",
        "  - **Device Memory**: Memory for the sparse matrix components and the result matrix `C_sparse` is allocated on the GPU.\n",
        "  - **Data Transfer**: Sparse matrix data is copied from host to device before kernel execution, and the resulting matrix `C_sparse` is copied back to the host afterward.\n",
        "\n",
        "### Result Verification and Output\n",
        "- After executing both the dense and sparse matrix multiplication kernels, the program prints a portion of the resulting matrices `C_dense` and `C_sparse`. This output allows for verification of the correctness of the matrix multiplication operations.\n",
        "\n",
        "### Memory Cleanup\n",
        "- The program ensures proper cleanup by freeing both host and device memory after the computation, preventing memory leaks and ensuring efficient use of resources.\n",
        "\n",
        "This code provides a practical example of how to implement and compare dense and sparse matrix multiplication on a GPU using CUDA, demonstrating the advantages of using sparse matrices when dealing with data that contains many zeroes.\n"
      ],
      "metadata": {
        "id": "kGE8j3DpwwKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the CUDA program\n",
        "!nvcc -lcusparse -o matrix_mul matrix_mul.cu\n"
      ],
      "metadata": {
        "id": "F2kshwuChp86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the CUDA program\n",
        "!./matrix_mul\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UseLLvt1hrMS",
        "outputId": "d919c4d1-7f89-45c2-f809-dcbb33068682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix C (Dense - showing a part of it):\n",
            "22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 \n",
            "44870328.000000 89740656.000000 134611216.000000 179481312.000000 44870328.000000 89740656.000000 134611216.000000 179481312.000000 44870328.000000 89740656.000000 \n",
            "67305608.000000 134611216.000000 201916704.000000 269222432.000000 67305608.000000 134611216.000000 201916704.000000 269222432.000000 67305608.000000 134611216.000000 \n",
            "22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 \n",
            "44870328.000000 89740656.000000 134611216.000000 179481312.000000 44870328.000000 89740656.000000 134611216.000000 179481312.000000 44870328.000000 89740656.000000 \n",
            "67305608.000000 134611216.000000 201916704.000000 269222432.000000 67305608.000000 134611216.000000 201916704.000000 269222432.000000 67305608.000000 134611216.000000 \n",
            "22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 \n",
            "44870328.000000 89740656.000000 134611216.000000 179481312.000000 44870328.000000 89740656.000000 134611216.000000 179481312.000000 44870328.000000 89740656.000000 \n",
            "67305608.000000 134611216.000000 201916704.000000 269222432.000000 67305608.000000 134611216.000000 201916704.000000 269222432.000000 67305608.000000 134611216.000000 \n",
            "22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 67305608.000000 89740656.000000 22435164.000000 44870328.000000 \n",
            "\n",
            "Result matrix C (Sparse - showing a part of it):\n",
            "3.000000 6.000000 9.000000 12.000000 3.000000 6.000000 9.000000 12.000000 3.000000 6.000000 \n",
            "4.500000 9.000000 13.500000 18.000000 4.500000 9.000000 13.500000 18.000000 4.500000 9.000000 \n",
            "6.000000 12.000000 18.000000 24.000000 6.000000 12.000000 18.000000 24.000000 6.000000 12.000000 \n",
            "7.500000 15.000000 22.500000 30.000000 7.500000 15.000000 22.500000 30.000000 7.500000 15.000000 \n",
            "9.000000 18.000000 27.000000 36.000000 9.000000 18.000000 27.000000 36.000000 9.000000 18.000000 \n",
            "10.500000 21.000000 31.500000 42.000000 10.500000 21.000000 31.500000 42.000000 10.500000 21.000000 \n",
            "12.000000 24.000000 36.000000 48.000000 12.000000 24.000000 36.000000 48.000000 12.000000 24.000000 \n",
            "13.500000 27.000000 40.500000 54.000000 13.500000 27.000000 40.500000 54.000000 13.500000 27.000000 \n",
            "15.000000 30.000000 45.000000 60.000000 15.000000 30.000000 45.000000 60.000000 15.000000 30.000000 \n",
            "16.500000 33.000000 49.500000 66.000000 16.500000 33.000000 49.500000 66.000000 16.500000 33.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Experiment with Dense and Sparse Matrix Multiplication in CUDA\n",
        "\n",
        "In this exercise, you will modify the existing CUDA program to better understand the difference between dense and sparse matrix multiplication. The program currently performs both dense and sparse matrix multiplication on a 512x512 matrix using CUDA.\n",
        "\n",
        "### Task 1: Modify the Sparse Matrix Structure\n",
        "The current sparse matrix is initialized with **3 non-zero entries per row**. Modify the program so that:\n",
        "1. The number of non-zero entries per row is **random** (between 1 and 5 non-zero elements per row).\n",
        "2. Observe how the output changes when the matrix sparsity varies.\n",
        "\n",
        "### Task 2: Change the Matrix Size\n",
        "The matrix size is currently defined as **512x512**. Modify the matrix size to:\n",
        "1. **Increase the matrix size** to 1024x1024.\n",
        "2. **Compare the performance** of dense and sparse matrix multiplication for different matrix sizes (e.g., 512x512 vs. 1024x1024).\n",
        "\n",
        "### Hints:\n",
        "- You can modify the number of non-zero entries per row by using a random function (e.g., `rand()`).\n",
        "- The matrix size can be modified by changing the value of the constant `N`.\n",
        "- Check the output and runtime for both dense and sparse matrices as you change these values.\n",
        "\n",
        "After making these changes, run the program and observe the results. This will help you understand the impact of sparsity on matrix multiplication performance and how CUDA handles larger matrices.\n"
      ],
      "metadata": {
        "id": "LKtkXVC-FhmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenACC"
      ],
      "metadata": {
        "id": "A4NBFxDIkO_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to OpenACC\n",
        "\n",
        "While CUDA is a powerful and flexible tool for GPU programming, it requires detailed management of the GPU's memory and parallelization. OpenACC is a higher-level programming model designed to simplify the process of parallelizing code. With OpenACC, you can accelerate your existing C, C++, and Fortran applications without requiring in-depth knowledge of GPU architecture.\n",
        "\n",
        "### Key Concepts in OpenACC\n",
        "\n",
        "- **Directives**: OpenACC uses compiler directives (pragmas) to specify which parts of the code should run on the GPU. This allows for incremental parallelization of existing code.\n",
        "- **Parallel Region**: A block of code that is executed by multiple threads in parallel on the GPU.\n",
        "- **Data Region**: Specifies the movement of data between the CPU and GPU memory.\n",
        "\n",
        "Let's take a look at a similar matrix multiplication example using OpenACC.\n"
      ],
      "metadata": {
        "id": "3O5QPvXsvz2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wget and the necessary dependencies\n",
        "!apt-get install -y wget build-essential\n",
        "\n",
        "# Download the NVIDIA HPC SDK\n",
        "!wget https://developer.download.nvidia.com/hpc-sdk/23.1/nvhpc_2023_231_Linux_x86_64_cuda_12.0.tar.gz -O nvhpc.tar.gz\n",
        "\n",
        "# Extract the downloaded tarball\n",
        "!tar -xzf nvhpc.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4i4HxRVkSCP",
        "outputId": "9de25deb-4bf8-47f6-b219-f20bbbb721a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "--2024-09-21 10:22:10--  https://developer.download.nvidia.com/hpc-sdk/23.1/nvhpc_2023_231_Linux_x86_64_cuda_12.0.tar.gz\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5084055249 (4.7G) [application/x-gzip]\n",
            "Saving to: ‘nvhpc.tar.gz’\n",
            "\n",
            "nvhpc.tar.gz        100%[===================>]   4.73G   119MB/s    in 41s     \n",
            "\n",
            "2024-09-21 10:22:51 (118 MB/s) - ‘nvhpc.tar.gz’ saved [5084055249/5084055249]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the extracted folder and run the install script non-interactively\n",
        "!cd nvhpc_2023_231_Linux_x86_64_cuda_12.0 && yes \"\" | ./install --installpath /usr/local/nvhpc --accept --silent\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMv6iELpmnsl",
        "outputId": "e5145049-2f0d-4c55-b7fe-9e4b71aac84d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Welcome to the NVIDIA HPC SDK Linux installer!\n",
            "\n",
            "You are installing NVIDIA HPC SDK 2023 version 23.1 for Linux_x86_64.\n",
            "Please note that all Trademarks and Marks are the properties\n",
            "of their respective owners.\n",
            "\n",
            "Press enter to continue...\n",
            "\n",
            "A network installation will save disk space by having only one copy of the\n",
            "compilers and most of the libraries for all compilers on the network, and\n",
            "the main installation needs to be done once for all systems on the network.\n",
            "\n",
            "1  Single system install\n",
            "2  Network install\n",
            "\n",
            "Please choose install option: \n",
            "\n",
            "Please specify the directory path under which the software will be installed.\n",
            "The default directory is /opt/nvidia/hpc_sdk, but you may install anywhere you wish,\n",
            "assuming you have permission to do so.\n",
            "\n",
            "Installation directory? [/opt/nvidia/hpc_sdk] \n",
            "\n",
            "Note: directory /opt/nvidia/hpc_sdk was created.\n",
            "\n",
            "Installing NVIDIA HPC SDK version 23.1 into /opt/nvidia/hpc_sdk\n",
            "Making symbolic links in /opt/nvidia/hpc_sdk/Linux_x86_64/2023\n",
            "\n",
            "generating environment modules for NV HPC SDK 23.1 ... done.\n",
            "Installation complete.\n",
            "HPC SDK successfully installed into /opt/nvidia/hpc_sdk\n",
            "\n",
            "If you use the Environment Modules package, that is, the module load\n",
            "command, the NVIDIA HPC SDK includes a script to set up the\n",
            "appropriate module files.\n",
            "\n",
            "% module load /opt/nvidia/hpc_sdk/modulefiles/nvhpc/23.1\n",
            "% module load nvhpc/23.1\n",
            "\n",
            "Alternatively, the shell environment may be initialized to use the HPC SDK.\n",
            "\n",
            "In csh, use these commands:\n",
            "\n",
            "% setenv MANPATH \"$MANPATH\":/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/man\n",
            "% set path = (/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/bin $path)\n",
            "\n",
            "In bash, sh, or ksh, use these commands:\n",
            "\n",
            "$ MANPATH=$MANPATH:/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/man; export MANPATH\n",
            "$ PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/bin:$PATH; export PATH\n",
            "\n",
            "Once the 64-bit compilers are available, you can make the OpenMPI\n",
            "commands and man pages accessible using these commands.\n",
            "\n",
            "% set path = (/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/comm_libs/mpi/bin $path)\n",
            "% setenv MANPATH \"$MANPATH\":/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/comm_libs/mpi/man\n",
            "\n",
            "And the equivalent in bash, sh, and ksh:\n",
            "\n",
            "$ export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/comm_libs/mpi/bin:$PATH\n",
            "$ export MANPATH=$MANPATH:/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/comm_libs/mpi/man\n",
            "\n",
            "Please check https://developer.nvidia.com for documentation,\n",
            "use of NVIDIA HPC SDK software, and other questions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Set up environment variables for the NVIDIA HPC SDK installed in /opt/nvidia/hpc_sdk\n",
        "os.environ['PATH'] = '/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/bin:' + os.environ['PATH']\n",
        "os.environ['LD_LIBRARY_PATH'] = '/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/lib:' + os.environ['LD_LIBRARY_PATH']\n"
      ],
      "metadata": {
        "id": "v_W_Hy0Qr9HN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if nvc is in the PATH\n",
        "!which nvc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIYBqrR8rq-d",
        "outputId": "dc3a7963-8b8f-43cc-d33f-23b7295e4e2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/nvidia/hpc_sdk/Linux_x86_64/23.1/compilers/bin/nvc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to OpenACC: Simple Vector Addition\n",
        "\n",
        "OpenACC is a directive-based programming model that simplifies parallel programming for multi-core CPUs and GPUs. With just a few directives, OpenACC allows us to offload computation to GPUs, without needing to manage the low-level details of GPU programming (like CUDA).\n",
        "\n",
        "### Key Concepts of OpenACC:\n",
        "1. **Directives**: OpenACC uses `#pragma acc` directives to indicate which parts of the code should be parallelized.\n",
        "2. **Parallelization**: We can parallelize loops easily using OpenACC, allowing multiple threads to work on different iterations of the loop simultaneously.\n",
        "3. **Data Transfer**: OpenACC handles data movement between the CPU (host) and the GPU (device) with `copy`, `copyin`, `copyout`, and `create`.\n",
        "\n",
        "### Simple Vector Addition:\n",
        "In this example, we will perform a simple vector addition. The program will:\n",
        "1. **Initialize two vectors** `A` and `B` on the CPU (host).\n",
        "2. **Use OpenACC directives** to parallelize the addition of these vectors on the GPU (device).\n",
        "3. **Store the result** in vector `C` and print the first few values of the result.\n",
        "\n",
        "### Task:\n",
        "Run the provided OpenACC code for vector addition using vectors of size 1024. Observe the result printed on the console. This will help you understand how OpenACC simplifies parallel programming.\n"
      ],
      "metadata": {
        "id": "rznzzLeFGQYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Writing the OpenACC C code to a file for simple vector addition\n",
        "openacc_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024  // Define the size of the vectors\n",
        "\n",
        "int main() {\n",
        "    // Allocate memory on the host\n",
        "    float *A = (float *)malloc(N * sizeof(float));\n",
        "    float *B = (float *)malloc(N * sizeof(float));\n",
        "    float *C = (float *)malloc(N * sizeof(float));\n",
        "\n",
        "    // Initialize vectors A and B with some values\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        A[i] = i * 1.0f;\n",
        "        B[i] = i * 2.0f;\n",
        "    }\n",
        "\n",
        "    // Perform vector addition using OpenACC\n",
        "    #pragma acc data copyin(A[0:N], B[0:N]), copyout(C[0:N])\n",
        "    {\n",
        "        #pragma acc parallel loop\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            C[i] = A[i] + B[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Print a portion of the result vector\n",
        "    printf(\"Result vector C (first 10 elements):\\\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%f + %f = %f\\\\n\", A[i], B[i], C[i]);\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Save the OpenACC code to a file\n",
        "with open('vector_add_openacc.c', 'w') as f:\n",
        "    f.write(openacc_code)\n",
        "\n",
        "# Step 3: Compile the OpenACC code using nvc (NVIDIA C Compiler) from the HPC SDK\n",
        "!nvc -acc -o vector_add_openacc vector_add_openacc.c -Minfo=accel\n",
        "\n",
        "# Step 4: Run the OpenACC program\n",
        "!./vector_add_openacc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL0NxtGcGQts",
        "outputId": "12e44d58-134a-4c2e-9427-4de3a66f725a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main:\n",
            "     21, Generating copyin(A[:1024]) [if not already present]\n",
            "         Generating copyout(C[:1024]) [if not already present]\n",
            "         Generating copyin(B[:1024]) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         23, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n",
            "Result vector C (first 10 elements):\n",
            "0.000000 + 0.000000 = 0.000000\n",
            "1.000000 + 2.000000 = 3.000000\n",
            "2.000000 + 4.000000 = 6.000000\n",
            "3.000000 + 6.000000 = 9.000000\n",
            "4.000000 + 8.000000 = 12.000000\n",
            "5.000000 + 10.000000 = 15.000000\n",
            "6.000000 + 12.000000 = 18.000000\n",
            "7.000000 + 14.000000 = 21.000000\n",
            "8.000000 + 16.000000 = 24.000000\n",
            "9.000000 + 18.000000 = 27.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to OpenACC: Data Management and Atomics\n",
        "\n",
        "In this section, we will explore how to use **data management** and **atomic operations** in OpenACC. These are crucial for efficiently managing memory transfers between the CPU and GPU, and for ensuring that multiple threads can safely update shared data.\n",
        "\n",
        "### Key Concepts of OpenACC:\n",
        "1. **Data Directives**: OpenACC uses data directives to control the movement of data between the CPU (host) and the GPU (device). Some important clauses include:\n",
        "   - **`copy`**: Allocates memory on the GPU, copies the data from the CPU to the GPU, and copies it back to the CPU after the computation is done.\n",
        "   - **`copyin`**: Copies data from the CPU to the GPU, but does not copy it back.\n",
        "   - **`copyout`**: Allocates memory on the GPU and copies the result back to the CPU after computation.\n",
        "2. **Atomic Operations**: Atomic operations ensure that updates to shared data by multiple threads are performed one at a time, preventing race conditions. In OpenACC, you can use `#pragma acc atomic` to enforce this.\n",
        "\n",
        "### Example: Safe Parallel Increment with Atomic Operations\n",
        "In this example, we will:\n",
        "1. **Initialize an array** and a shared counter.\n",
        "2. **Use OpenACC to parallelize the loop** that increments the shared counter based on array values.\n",
        "3. Use **atomic operations** to ensure that the counter is safely updated by multiple threads.\n",
        "\n",
        "### Task:\n",
        "Run the provided OpenACC code, which uses `copyin`, `copyout`, and `copy` clauses for data management and `#pragma acc atomic` for safe updates to the shared counter. Observe how atomic operations work to avoid race conditions.\n"
      ],
      "metadata": {
        "id": "5cQHpUCpHOim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Writing the OpenACC C code to a file for data management and atomic operations\n",
        "openacc_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1000  // Define the size of the array\n",
        "\n",
        "int main() {\n",
        "    // Allocate memory on the host\n",
        "    int *A = (int *)malloc(N * sizeof(int));\n",
        "    int sum = 0;  // Shared variable for atomic increment\n",
        "\n",
        "    // Initialize the array A\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        A[i] = 1;  // All values are set to 1 for easy summing\n",
        "    }\n",
        "\n",
        "    // Perform parallel summing using OpenACC with atomic operation\n",
        "    #pragma acc data copyin(A[0:N]), copy(sum)\n",
        "    {\n",
        "        #pragma acc parallel loop\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            #pragma acc atomic\n",
        "            sum += A[i];  // Atomic operation to safely increment shared variable\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Print the result\n",
        "    printf(\"The total sum of the array is: %d\\\\n\", sum);\n",
        "\n",
        "    // Free memory\n",
        "    free(A);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Save the OpenACC code to a file\n",
        "with open('data_mgmt_atomic_openacc.c', 'w') as f:\n",
        "    f.write(openacc_code)\n",
        "\n",
        "# Step 3: Compile the OpenACC code using nvc (NVIDIA C Compiler) from the HPC SDK\n",
        "!nvc -acc -o data_mgmt_atomic_openacc data_mgmt_atomic_openacc.c -Minfo=accel\n",
        "\n",
        "# Step 4: Run the OpenACC program\n",
        "!./data_mgmt_atomic_openacc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_rg3xrOHQrB",
        "outputId": "6ea872a3-00db-47c0-efb0-37c8ca300fa7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main:\n",
            "     19, Generating copy(sum) [if not already present]\n",
            "         Generating copyin(A[:1000]) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         21, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n",
            "The total sum of the array is: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Experiment with Data Management and Atomic Operations in OpenACC\n",
        "\n",
        "In this exercise, you will modify the existing OpenACC program to better understand how atomic operations and data management work.\n",
        "\n",
        "### Task 1: Remove Atomic and Observe the Impact\n",
        "The current program uses **`#pragma acc atomic`** to ensure that the variable `sum` is incremented safely when multiple threads are updating it. Modify the code by:\n",
        "1. **Removing the `#pragma acc atomic` directive**.\n",
        "2. Run the program and observe what happens when atomic operations are removed. Do you get the same sum? What issues arise?\n",
        "\n",
        "### Task 2: Parallelize Initialization of the Array\n",
        "Currently, the array `A` is initialized on the CPU in a serial manner. Modify the program so that:\n",
        "1. The initialization of array `A` is done in parallel using OpenACC.\n",
        "2. Use **`#pragma acc parallel loop`** to parallelize the loop that initializes `A`.\n",
        "\n",
        "### Hints:\n",
        "- Removing the atomic directive may cause race conditions. Observe how the output changes and consider why the result is different.\n",
        "- Use **data management clauses** like `copyin` and `copyout` to manage the data transfer between the host (CPU) and device (GPU) when initializing the array in parallel.\n",
        "\n",
        "After making these changes, run the program and observe how removing atomic operations and parallelizing the array initialization impact the result and performance.\n"
      ],
      "metadata": {
        "id": "H5WQHZlCIg_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenACC: Understanding `parallel` vs `kernels`\n",
        "\n",
        "OpenACC provides two main directives to parallelize code: **`parallel`** and **`kernels`**. Both are used to execute code on the GPU, but they serve slightly different purposes.\n",
        "\n",
        "### Key Differences:\n",
        "1. **`parallel` Directive**:\n",
        "   - The `parallel` directive gives you **explicit control** over which parts of the code are parallelized.\n",
        "   - You need to manually specify the loops or blocks of code that should be run in parallel.\n",
        "   - Suitable when you know exactly how you want to parallelize your loops.\n",
        "\n",
        "2. **`kernels` Directive**:\n",
        "   - The `kernels` directive lets the compiler automatically decide how to parallelize the code.\n",
        "   - The compiler analyzes the code and identifies parallel loops.\n",
        "   - Suitable for more **general parallelization** where you trust the compiler to do the work.\n",
        "\n",
        "### Example: Vector Addition with `parallel` and `kernels`\n",
        "In this example, we will:\n",
        "1. **Use the `parallel` directive** to parallelize the vector addition manually.\n",
        "2. **Use the `kernels` directive** to let the compiler decide the parallelization.\n",
        "\n",
        "Both implementations perform the same operation, but the choice of directive determines how much control you have over the parallelization process.\n",
        "\n",
        "### Task:\n",
        "Run the provided OpenACC code to see how vector addition is performed using both `parallel` and `kernels`. Compare the outputs and observe how each directive behaves.\n"
      ],
      "metadata": {
        "id": "u6K3pn2DHbPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Writing the OpenACC C code to a file for parallel vs kernels\n",
        "openacc_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024  // Define the size of the vectors\n",
        "\n",
        "int main() {\n",
        "    // Allocate memory on the host\n",
        "    float *A = (float *)malloc(N * sizeof(float));\n",
        "    float *B = (float *)malloc(N * sizeof(float));\n",
        "    float *C_parallel = (float *)malloc(N * sizeof(float));\n",
        "    float *C_kernels = (float *)malloc(N * sizeof(float));\n",
        "\n",
        "    // Initialize vectors A and B with some values\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        A[i] = i * 1.0f;\n",
        "        B[i] = i * 2.0f;\n",
        "    }\n",
        "\n",
        "    // Vector addition using OpenACC with the 'parallel' directive\n",
        "    #pragma acc data copyin(A[0:N], B[0:N]), copyout(C_parallel[0:N])\n",
        "    {\n",
        "        #pragma acc parallel loop\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            C_parallel[i] = A[i] + B[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Vector addition using OpenACC with the 'kernels' directive\n",
        "    #pragma acc data copyin(A[0:N], B[0:N]), copyout(C_kernels[0:N])\n",
        "    {\n",
        "        #pragma acc kernels\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            C_kernels[i] = A[i] + B[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Print a portion of the result vectors to compare\n",
        "    printf(\"Result of vector addition using 'parallel' (first 10 elements):\\\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%f + %f = %f\\\\n\", A[i], B[i], C_parallel[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"\\\\nResult of vector addition using 'kernels' (first 10 elements):\\\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%f + %f = %f\\\\n\", A[i], B[i], C_kernels[i]);\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C_parallel);\n",
        "    free(C_kernels);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Save the OpenACC code to a file\n",
        "with open('parallel_vs_kernels_openacc.c', 'w') as f:\n",
        "    f.write(openacc_code)\n",
        "\n",
        "# Step 3: Compile the OpenACC code using nvc (NVIDIA C Compiler) from the HPC SDK\n",
        "!nvc -acc -o parallel_vs_kernels_openacc parallel_vs_kernels_openacc.c -Minfo=accel\n",
        "\n",
        "# Step 4: Run the OpenACC program\n",
        "!./parallel_vs_kernels_openacc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FmX_PljHbmZ",
        "outputId": "943d2384-2056-4124-a238-7700937fb144"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main:\n",
            "     22, Generating copyin(A[:1024],B[:1024]) [if not already present]\n",
            "         Generating copyout(C_parallel[:1024]) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         24, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n",
            "     31, Generating copyin(A[:1024]) [if not already present]\n",
            "         Generating copyout(C_kernels[:1024]) [if not already present]\n",
            "         Generating copyin(B[:1024]) [if not already present]\n",
            "     33, Complex loop carried dependence of B->,A-> prevents parallelization\n",
            "         Loop carried dependence of C_kernels-> prevents parallelization\n",
            "         Loop carried backward dependence of C_kernels-> prevents vectorization\n",
            "         Accelerator serial kernel generated\n",
            "         Generating NVIDIA GPU code\n",
            "         33, #pragma acc loop seq\n",
            "     33, Complex loop carried dependence of B->,A-> prevents parallelization\n",
            "         Loop carried backward dependence of C_kernels-> prevents vectorization\n",
            "Result of vector addition using 'parallel' (first 10 elements):\n",
            "0.000000 + 0.000000 = 0.000000\n",
            "1.000000 + 2.000000 = 3.000000\n",
            "2.000000 + 4.000000 = 6.000000\n",
            "3.000000 + 6.000000 = 9.000000\n",
            "4.000000 + 8.000000 = 12.000000\n",
            "5.000000 + 10.000000 = 15.000000\n",
            "6.000000 + 12.000000 = 18.000000\n",
            "7.000000 + 14.000000 = 21.000000\n",
            "8.000000 + 16.000000 = 24.000000\n",
            "9.000000 + 18.000000 = 27.000000\n",
            "\n",
            "Result of vector addition using 'kernels' (first 10 elements):\n",
            "0.000000 + 0.000000 = 0.000000\n",
            "1.000000 + 2.000000 = 3.000000\n",
            "2.000000 + 4.000000 = 6.000000\n",
            "3.000000 + 6.000000 = 9.000000\n",
            "4.000000 + 8.000000 = 12.000000\n",
            "5.000000 + 10.000000 = 15.000000\n",
            "6.000000 + 12.000000 = 18.000000\n",
            "7.000000 + 14.000000 = 21.000000\n",
            "8.000000 + 16.000000 = 24.000000\n",
            "9.000000 + 18.000000 = 27.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Matrix Multiplication using OpenACC\n",
        "\n",
        "In this section, we will explore how to perform matrix multiplication using OpenACC, a programming model designed for parallel computing on multi-core CPUs and GPUs. OpenACC provides simple directives for accelerating code without needing to manage low-level details like memory transfers between the CPU and GPU.\n",
        "\n",
        "### Key Concepts of OpenACC:\n",
        "1. **Directives**: OpenACC uses compiler directives (e.g., `#pragma acc`) to specify which parts of the code should be parallelized and run on the GPU.\n",
        "2. **Data Management**: OpenACC allows developers to manage data transfer between the CPU (host) and GPU (device) using `copyin`, `copyout`, and `copy` clauses.\n",
        "3. **Parallel Loops**: OpenACC can parallelize loops automatically using `#pragma acc parallel loop`.\n",
        "\n",
        "### Matrix Multiplication Overview:\n",
        "In this example, we will perform matrix multiplication with OpenACC using a 512x512 matrix. The steps are as follows:\n",
        "1. Allocate and initialize the matrices `A` and `B`.\n",
        "2. Use OpenACC directives to perform matrix multiplication on the GPU.\n",
        "3. Copy the result back to the CPU and print a part of the resulting matrix `C`.\n",
        "\n",
        "### Task:\n",
        "Run the provided OpenACC code for matrix multiplication using a 512x512 matrix and observe the result printed on the console (a part of the resulting matrix). This will help you understand how OpenACC simplifies parallel programming.\n"
      ],
      "metadata": {
        "id": "QTFiaRVNGG2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openacc_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 512  // Define the size of the matrix\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    float *A = (float *)malloc(size);\n",
        "    float *B = (float *)malloc(size);\n",
        "    float *C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize matrices A and B with varying values\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        int row = i / N;\n",
        "        int col = i % N;\n",
        "        A[i] = (float)((row + 1) * (col + 2)) * 0.5f;  // Initialize A with values based on row and column\n",
        "        B[i] = (float)((col + 1) * (row + 2)) * 0.3f;  // Initialize B with values based on row and column\n",
        "    }\n",
        "\n",
        "    // Perform matrix multiplication using OpenACC\n",
        "    #pragma acc data copyin(A[0:N*N], B[0:N*N]), copyout(C[0:N*N])\n",
        "    {\n",
        "        #pragma acc parallel loop collapse(2)\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                float sum = 0.0f;\n",
        "                for (int k = 0; k < N; k++) {\n",
        "                    sum += A[i * N + k] * B[k * N + j];\n",
        "                }\n",
        "                C[i * N + j] = sum;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Print a portion of the result matrix\n",
        "    printf(\"Result matrix C (only showing a part of it):\\\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        for (int j = 0; j < 10; j++) {\n",
        "            printf(\"%f \", C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the OpenACC code to a file\n",
        "with open('matrix_mul_openacc.c', 'w') as f:\n",
        "    f.write(openacc_code)\n"
      ],
      "metadata": {
        "id": "JGgOfJcIkXqG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Experiment with OpenACC Matrix Multiplication\n",
        "\n",
        "In this exercise, you will modify the existing OpenACC matrix multiplication code to better understand how OpenACC handles parallelization and performance.\n",
        "\n",
        "### Task 1: Modify the Block Size\n",
        "The current OpenACC code uses a **`collapse(2)`** clause to parallelize the outer two loops of matrix multiplication. Modify the code so that:\n",
        "1. **Remove the `collapse(2)` clause** and parallelize just the outer loop (i.e., only the row loop).\n",
        "2. Run the program and compare the performance and results. Does removing the `collapse(2)` affect the computation or speed?\n",
        "\n",
        "### Task 2: Test with Different Matrix Sizes\n",
        "The matrix size is currently defined as **512x512**. Modify the matrix size to:\n",
        "1. **Increase the matrix size to 1024x1024**.\n",
        "2. Run the program and observe the difference in runtime with a larger matrix. How does the change in matrix size affect the performance?\n",
        "\n",
        "### Hints:\n",
        "- You can modify the matrix size by changing the value of the constant **`N`**.\n",
        "- Observe the difference between parallelizing the outer loop and collapsing two loops. This will help you understand how loop parallelization affects performance.\n",
        "\n",
        "After making these changes, run the program and observe how the modifications impact both the runtime and the output. This exercise will help you learn how OpenACC parallelizes loops and how matrix size influences performance.\n"
      ],
      "metadata": {
        "id": "P71RETB0IZd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the OpenACC Code for Matrix Multiplication\n",
        "\n",
        "This OpenACC program demonstrates matrix multiplication, a fundamental operation in many scientific computing tasks, using GPU acceleration. OpenACC is a user-friendly directive-based programming model that allows developers to parallelize code easily and target accelerators like GPUs without deep knowledge of GPU architecture.\n",
        "\n",
        "### Matrix Initialization\n",
        "- **Matrix Size and Memory Allocation**:\n",
        "  - The matrix size `N` is defined as 512, resulting in matrices `A`, `B`, and `C` of size 512x512.\n",
        "  - Memory for these matrices is allocated on the host using `malloc`, with each matrix occupying `N*N*sizeof(float)` bytes.\n",
        "  \n",
        "- **Initializing Matrices**:\n",
        "  - Matrices `A` and `B` are initialized with varying values based on their row and column indices. This variation ensures that each element in the matrices has a unique value, allowing for meaningful computation and results.\n",
        "  - Matrix `A` is filled with values calculated as `((row + 1) * (col + 2)) * 0.5f`.\n",
        "  - Matrix `B` is initialized with values calculated as `((col + 1) * (row + 2)) * 0.3f`.\n",
        "\n",
        "### Parallel Matrix Multiplication with OpenACC\n",
        "- **OpenACC Directives**:\n",
        "  - The `#pragma acc data` directive is used to manage data transfer between the host and the device (GPU). It ensures that matrices `A` and `B` are copied to the device before the computation, and matrix `C` is copied back to the host after the computation.\n",
        "  - The `#pragma acc parallel loop collapse(2)` directive specifies that the following nested loops should be parallelized, allowing multiple threads on the GPU to perform the matrix multiplication concurrently. The `collapse(2)` clause indicates that both loops should be parallelized together, enabling efficient use of the GPU's parallel architecture.\n",
        "\n",
        "- **Matrix Multiplication**:\n",
        "  - The nested loops iterate over the rows of `A` and columns of `B` to compute each element of the result matrix `C`. Each element `C[i * N + j]` is calculated as the dot product of the `i-th` row of `A` and the `j-th` column of `B`.\n",
        "  - The inner loop accumulates the product of corresponding elements from `A` and `B`, storing the sum in `C`.\n",
        "\n",
        "### Result Display and Memory Cleanup\n",
        "- **Output**:\n",
        "  - After computation, the program prints a portion of the result matrix `C` (the top-left 10x10 submatrix). This partial display allows for quick verification of the computation's correctness.\n",
        "  \n",
        "- **Memory Management**:\n",
        "  - The program concludes by freeing the allocated memory for matrices `A`, `B`, and `C` on the host, ensuring efficient use of resources and preventing memory leaks.\n",
        "\n",
        "This code provides a practical example of how to use OpenACC for parallelizing a common computational task (matrix multiplication) and demonstrates how OpenACC simplifies the process of leveraging GPU acceleration.\n"
      ],
      "metadata": {
        "id": "uOgHQlm6w2Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the OpenACC Program\n",
        "\n",
        "Now that we've written our OpenACC code for matrix multiplication, let's compile and run it. The OpenACC program should produce a result matrix similar to the CUDA version but with potentially simpler code.\n",
        "\n",
        "OpenACC abstracts much of the complexity of GPU programming, making it easier to parallelize existing C or Fortran code. Let's see how it performs.\n"
      ],
      "metadata": {
        "id": "dVQY_rl8v8Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the OpenACC code using nvc (NVIDIA C Compiler) from the HPC SDK\n",
        "!nvc -acc -o matrix_mul_openacc matrix_mul_openacc.c -Minfo=accel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsftQsGtkZmm",
        "outputId": "23deccaa-0f20-4bd6-e5d5-11d74a1b078f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main:\n",
            "     25, Generating copyin(A[:262144]) [if not already present]\n",
            "         Generating copyout(C[:262144]) [if not already present]\n",
            "         Generating copyin(B[:262144]) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         27, #pragma acc loop gang collapse(2) /* blockIdx.x */\n",
            "         28,   /* blockIdx.x collapsed */\n",
            "         30, #pragma acc loop vector(128) /* threadIdx.x */\n",
            "             Generating implicit reduction(+:sum)\n",
            "     30, Loop is parallelizable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the OpenACC program\n",
        "!./matrix_mul_openacc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMsJzBSckbdG",
        "outputId": "bb35b83c-4526-4fe6-9c68-9b84ce451a00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix C (only showing a part of it):\n",
            "6770035.000000 13540070.000000 20310106.000000 27080140.000000 33850176.000000 40620212.000000 47390248.000000 54160280.000000 60930320.000000 67700352.000000 \n",
            "13540070.000000 27080140.000000 40620212.000000 54160280.000000 67700352.000000 81240424.000000 94780496.000000 108320560.000000 121860640.000000 135400704.000000 \n",
            "20310108.000000 40620216.000000 60930320.000000 81240432.000000 101550528.000000 121860640.000000 142170736.000000 162480864.000000 182790944.000000 203101056.000000 \n",
            "27080140.000000 54160280.000000 81240424.000000 108320560.000000 135400704.000000 162480848.000000 189560992.000000 216641120.000000 243721280.000000 270801408.000000 \n",
            "33850176.000000 67700352.000000 101550528.000000 135400704.000000 169250880.000000 203101056.000000 236951232.000000 270801408.000000 304651584.000000 338501760.000000 \n",
            "40620216.000000 81240432.000000 121860640.000000 162480864.000000 203101056.000000 243721280.000000 284341472.000000 324961728.000000 365581888.000000 406202112.000000 \n",
            "47390248.000000 94780496.000000 142170752.000000 189560992.000000 236951232.000000 284341504.000000 331731712.000000 379121984.000000 426512224.000000 473902464.000000 \n",
            "54160280.000000 108320560.000000 162480848.000000 216641120.000000 270801408.000000 324961696.000000 379121984.000000 433282240.000000 487442560.000000 541602816.000000 \n",
            "60930320.000000 121860640.000000 182790944.000000 243721280.000000 304651584.000000 365581888.000000 426512256.000000 487442560.000000 548372864.000000 609303168.000000 \n",
            "67700352.000000 135400704.000000 203101056.000000 270801408.000000 338501760.000000 406202112.000000 473902464.000000 541602816.000000 609303168.000000 677003520.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing CUDA and OpenACC\n",
        "\n",
        "Both CUDA and OpenACC are powerful tools for leveraging the computational power of GPUs, but they serve different purposes and target different audiences:\n",
        "\n",
        "- **CUDA**: Offers more control over the hardware and is ideal for developers who need fine-grained optimization and are familiar with GPU architecture. It requires manual management of memory and parallelization, but this can lead to highly optimized code.\n",
        "\n",
        "- **OpenACC**: Provides a higher-level, more abstracted approach to GPU programming. It's easier to use for those who want to accelerate their applications without diving deeply into the specifics of GPU hardware. OpenACC is often used to incrementally parallelize existing applications.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Flexibility vs. Ease of Use**: CUDA is more flexible but requires more effort, while OpenACC is easier to use but might not offer the same level of optimization.\n",
        "- **Learning Curve**: CUDA has a steeper learning curve compared to OpenACC.\n",
        "- **Performance**: Depending on the application and how well the code is optimized, CUDA might offer better performance, but OpenACC can still deliver significant speedups with much less effort.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In this session, we explored the basics of GPU computing, focusing on two popular approaches: CUDA and OpenACC. We've implemented matrix multiplication in both frameworks and compared their usage. Understanding both CUDA and OpenACC allows you to choose the best tool for your specific needs, whether that’s maximum performance or ease of development.\n",
        "\n",
        "Continue experimenting with these examples and explore how you can leverage GPU computing for your projects!\n"
      ],
      "metadata": {
        "id": "gz6LLkuxv-5K"
      }
    }
  ]
}
