{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzLmH2GLx159"
   },
   "source": [
    "# 2.3 MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaiQn6fCz5Mp"
   },
   "source": [
    "# Introduction to MPI in High-Performance Computing (HPC)\n",
    "\n",
    "High-Performance Computing (HPC) often involves running computational tasks that require massive parallelism across many processors. To achieve this, it's essential to use tools that can effectively manage communication between these processors.\n",
    "\n",
    "**Message Passing Interface (MPI)** is a standardized and portable message-passing system designed to function on a wide variety of parallel computing architectures. MPI is one of the cornerstones of parallel computing, particularly in distributed-memory systems, where each processor has its own memory and processors communicate by passing messages.\n",
    "\n",
    "In this lesson, we'll delve into the basics of MPI programming. You'll learn how to develop parallel applications that can efficiently communicate and share data across multiple processors. We'll explore MPI's core concepts through hands-on examples, starting with a simple yet powerful exercise known as the \"ping-pong\" example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x08dzaR1z7NZ"
   },
   "source": [
    "# Overview of MPI Concepts\n",
    "\n",
    "Before diving into the code, it's important to understand some key concepts in MPI:\n",
    "\n",
    "- **Processes and Ranks**: In MPI, a process is an instance of a program running on a processor. Each process is assigned a unique identifier called a \"rank.\" The rank is used to identify and communicate with other processes.\n",
    "  \n",
    "- **Communicators**: A communicator defines a group of processes that can communicate with each other. The default communicator `MPI_COMM_WORLD` includes all the processes launched by the MPI program.\n",
    "\n",
    "- **Point-to-Point Communication**: This involves the direct sending and receiving of messages between two processes. MPI provides functions such as `MPI_Send` and `MPI_Recv` to facilitate this communication.\n",
    "\n",
    "- **Collective Communication**: This involves communication patterns where data is distributed among multiple processes or gathered from them. Examples include broadcast, scatter, and gather operations.\n",
    "\n",
    "These concepts form the foundation for writing parallel applications using MPI. Now, let's see how these concepts are applied in practice with the ping-pong example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJn_JJlLziBk"
   },
   "source": [
    "# Setting Up MPI in Google Colab\n",
    "\n",
    "To run MPI programs, we first need to set up the MPI environment in Google Colab. We will use Open MPI, a popular implementation of the MPI standard. The first step is to install the necessary MPI libraries and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knlix2Waxzox",
    "outputId": "24bbcd69-e7d8-4760-bdfc-95f3e8921810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: apt-get: command not found\n",
      "/bin/bash: line 1: apt-get: command not found\n"
     ]
    }
   ],
   "source": [
    "# Install MPI (mpich) in Google Colab\n",
    "!apt-get update -y\n",
    "!apt-get install -y mpich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xuAiAn5df-te"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
    "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e03ilEFgDbm",
    "outputId": "9e5bc207-d70b-4589-a3be-71ac98a0aa97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C program written to 'mpi_program.c'\n",
      "C program compiled successfully\n",
      "Hello from process 1 of 4\n",
      "Hello from process 2 of 4\n",
      "Hello from process 3 of 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# 1. Write the C code to a file\n",
    "c_code = \"\"\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <string.h>\n",
    "\n",
    "int main(int argc,char **argv)\n",
    "{\n",
    "    int rank, size;\n",
    "    MPI_Init(&argc,&argv);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
    "\n",
    "    int message[2];    // buffer for sending and receiving messages\n",
    "    int dest, src;     // destination and source process variables\n",
    "    int tag = 0;\n",
    "    MPI_Status status;\n",
    "\n",
    "    // This example has to be run on more than one process\n",
    "    if (size == 1) {\n",
    "        printf(\"This example requires >1 process to execute\\\\n\");\n",
    "        MPI_Finalize();\n",
    "        exit(0);\n",
    "    }\n",
    "\n",
    "    if (rank != 0) {\n",
    "        // If not rank 0, send message to rank 0\n",
    "        message[0] = rank;\n",
    "        message[1] = size;\n",
    "        dest = 0;  // send all messages to rank 0\n",
    "        MPI_Send(message, 2, MPI_INT, dest, tag, MPI_COMM_WORLD);\n",
    "    } else {\n",
    "        // If rank 0, receive messages from everybody else\n",
    "        for (src = 1; src < size; src++) {\n",
    "            MPI_Recv(message, 2, MPI_INT, src, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n",
    "            // prints message just received. Notice it will print in rank\n",
    "            // order since the loop is in rank order.\n",
    "            printf(\"Hello from process %d of %d\\\\n\", message[0], message[1]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Write the C code to a file\n",
    "with open(\"mpi_program.c\", \"w\") as c_file:\n",
    "    c_file.write(c_code)\n",
    "\n",
    "print(\"C program written to 'mpi_program.c'\")\n",
    "\n",
    "# 2. Compile the C code using mpicc\n",
    "compile_command = [\"mpicc\", \"-o\", \"mpi_program\", \"mpi_program.c\"]\n",
    "subprocess.run(compile_command, check=True)\n",
    "print(\"C program compiled successfully\")\n",
    "\n",
    "# 3. Run the compiled program with 4 nodes and allow oversubscription\n",
    "run_command = [\"mpirun\", \"--oversubscribe\", \"-np\", \"4\", \"./mpi_program\"]\n",
    "try:\n",
    "    result = subprocess.run(run_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())  # Output from the command\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred while running MPI program:\", e.stderr.decode())  # Print error output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz55jagIz-RB"
   },
   "source": [
    "# The Ping-Pong Example\n",
    "\n",
    "The \"ping-pong\" program is a classic introductory example in MPI programming. It demonstrates how two processes can communicate by passing a message (or \"ping-pong ball\") back and forth. The program consists of two main steps:\n",
    "\n",
    "1. **Initialization**: Both processes initialize MPI, get their ranks, and determine who they will communicate with.\n",
    "\n",
    "2. **Message Passing**: The two processes take turns sending and receiving a message, incrementing a counter each time the message is passed. The process with rank 0 starts by sending the message to process 1. The message continues to be passed back and forth until a predefined count is reached.\n",
    "\n",
    "This example helps you understand the basic mechanics of point-to-point communication in MPI, including how messages are sent and received and how the rank of a process determines its role in the communication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v9D0EVJIypGn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
    "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9bdZVqt2yAhd"
   },
   "outputs": [],
   "source": [
    "# Step 1: Writing the MPI ping-pong example to a file\n",
    "mpi_code = \"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <unistd.h> // For sleep\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int world_size;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "\n",
    "    // Ensure that there are at least 2 processes\n",
    "    if (world_size < 2) {\n",
    "        fprintf(stderr, \"World size must be greater than 1 for %s\\\\n\", argv[0]);\n",
    "        MPI_Abort(MPI_COMM_WORLD, 1);\n",
    "    }\n",
    "\n",
    "    int ping_pong_count = 0;\n",
    "    int partner_rank = (world_rank + 1) % 2;\n",
    "    const int MAX_COUNT = 10;  // Reduced number of ping-pong iterations\n",
    "\n",
    "    while (ping_pong_count < MAX_COUNT) {\n",
    "        if (world_rank == ping_pong_count % 2) {\n",
    "            // Increment the count before sending\n",
    "            ping_pong_count++;\n",
    "            MPI_Send(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n",
    "            printf(\"Process %d sent ping_pong_count %d to process %d\\\\n\", world_rank, ping_pong_count, partner_rank);\n",
    "            fflush(stdout); // Ensure output is flushed\n",
    "            sleep(1);  // Small delay to avoid flooding\n",
    "        } else {\n",
    "            MPI_Recv(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "            printf(\"Process %d received ping_pong_count %d from process %d\\\\n\", world_rank, ping_pong_count, partner_rank);\n",
    "            fflush(stdout); // Ensure output is flushed\n",
    "        }\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Save the MPI code to a file\n",
    "with open('ping_pong.c', 'w') as f:\n",
    "    f.write(mpi_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfdZKptu1lrj"
   },
   "source": [
    "## MPI Ping Pong Example Explained\n",
    "\n",
    "The code provided implements a simple MPI (Message Passing Interface) \"ping pong\" program. This program demonstrates the basic concepts of point-to-point communication between two processes in an MPI environment. Below is a detailed explanation of the code.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "1. **Initialization**:\n",
    "   - `MPI_Init(NULL, NULL);`: Initializes the MPI environment. This must be called before any other MPI function. The `argc` and `argv` parameters allow MPI to take command-line arguments if needed.\n",
    "\n",
    "2. **Rank and Size**:\n",
    "   - `MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);`: Determines the rank of the calling process in the communicator `MPI_COMM_WORLD`. The rank is the unique ID assigned to each process within the communicator, starting from 0.\n",
    "   - `MPI_Comm_size(MPI_COMM_WORLD, &world_size);`: Determines the number of processes in the communicator `MPI_COMM_WORLD`.\n",
    "\n",
    "3. **Error Checking**:\n",
    "   - The program assumes at least two processes for this example. If fewer than two processes are available, the program prints an error message and aborts using `MPI_Abort`.\n",
    "\n",
    "4. **Ping Pong Logic**:\n",
    "   - The `ping_pong_count` variable tracks the number of messages sent back and forth.\n",
    "   - `partner_rank = (world_rank + 1) % 2;`: Each process calculates the rank of its partner process. For two processes, rank 0's partner is rank 1, and rank 1's partner is rank 0.\n",
    "   - The `while` loop continues until `ping_pong_count` reaches 10. The processes alternate sending and receiving the `ping_pong_count` value.\n",
    "     - **Sending**: If the current process's rank matches the current `ping_pong_count % 2`, it increments the `ping_pong_count`, sends it to the partner process, and prints a message.\n",
    "     - **Receiving**: If the current process's rank does not match `ping_pong_count % 2`, it waits to receive the `ping_pong_count` from the partner process and then prints a message.\n",
    "\n",
    "5. **Finalization**:\n",
    "   - `MPI_Finalize();`: Cleans up the MPI environment. No MPI functions should be called after this.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **MPI_Comm_rank** and **MPI_Comm_size** are essential for identifying the process and determining the total number of processes involved.\n",
    "- **MPI_Send** and **MPI_Recv** are basic point-to-point communication functions, used here to send and receive the `ping_pong_count` variable between the two processes.\n",
    "- **Synchronization**: The processes are synchronized via alternating sends and receives, ensuring that the ping pong count is passed back and forth correctly.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "When you run this program with two processes, the output will look something like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4YOMgP63yD6k"
   },
   "outputs": [],
   "source": [
    "# Compile the MPI program\n",
    "!mpicc -o ping_pong ping_pong.c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ji8gZ2XHy23P",
    "outputId": "bac07531-9583-4578-b6a9-33b80beda1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 sent ping_pong_count 1 to process 1\n",
      "Process 1 received ping_pong_count 1 from process 0\n",
      "Process 1 sent ping_pong_count 2 to process 0\n",
      "Process 0 received ping_pong_count 2 from process 1\n",
      "Process 0 sent ping_pong_count 3 to process 1\n",
      "Process 1 received ping_pong_count 3 from process 0\n",
      "Process 1 sent ping_pong_count 4 to process 0\n",
      "Process 0 received ping_pong_count 4 from process 1\n",
      "Process 0 sent ping_pong_count 5 to process 1\n",
      "Process 1 received ping_pong_count 5 from process 0\n",
      "Process 1 sent ping_pong_count 6 to process 0\n",
      "Process 1 received ping_pong_count 7 from process 0\n",
      "Process 1 sent ping_pong_count 8 to process 0\n",
      "Process 0 received ping_pong_count 6 from process 1\n",
      "Process 0 sent ping_pong_count 7 to process 1\n",
      "Process 1 received ping_pong_count 9 from process 0\n",
      "Process 1 sent ping_pong_count 10 to process 0\n",
      "Process 0 received ping_pong_count 8 from process 1\n",
      "Process 0 sent ping_pong_count 9 to process 1\n",
      "Process 0 received ping_pong_count 10 from process 1\n"
     ]
    }
   ],
   "source": [
    "# Run the MPI program with 2 processes and allow running as root, with oversubscription\n",
    "!mpirun --oversubscribe -np 2 ./ping_pong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O5wpKDy0Eor"
   },
   "source": [
    "# MPI Dot Product Example\n",
    "\n",
    "In this example, we compute the dot product of two vectors distributed across multiple processes using MPI. Each process computes a portion of the dot product (local dot product), and the final result is obtained by reducing (summing) all the local dot products at the root process.\n",
    "\n",
    "### Key Concepts\n",
    "1. **Process Distribution**: Each process holds a portion of the vectors `a` and `b`, and computes the dot product of its portion. The size of each local vector is constant across all processes.\n",
    "2. **MPI_Reduce**: This MPI function is used to collect the partial dot products from all processes and sum them at the root process.\n",
    "\n",
    "### Steps in the Code\n",
    "1. **Vector Initialization**: Each process initializes its portion of the vectors `a` and `b` based on its rank.\n",
    "2. **Partial Dot Product Calculation**: Each process computes the dot product for its portion of the vectors.\n",
    "3. **Reduction**: The partial dot products from all processes are reduced (summed) to compute the final dot product at the root process.\n",
    "4. **Final Output**: The root process prints the final dot product.\n",
    "\n",
    "### Code Walkthrough\n",
    "- **MPI Initialization**: We initialize MPI using `MPI_Init()`, and each process determines its rank and the total number of processes.\n",
    "- **Vector Initialization**: Each process allocates memory for its local portion of the vectors `a` and `b` and initializes them based on its rank.\n",
    "- **Partial Dot Product**: Each process calculates the dot product of its local vectors.\n",
    "- **MPI_Reduce**: The partial dot products are summed up at the root process using `MPI_Reduce()`.\n",
    "- **Final Output**: The root process prints the final dot product.\n",
    "\n",
    "### Task:\n",
    "Run this code using 4 processes and check the result.\n",
    "\n",
    "### MPI Code Compilation and Execution in Jupyter\n",
    "The following code will write the MPI program to a file, compile it using `mpicc`, and run it with oversubscription in case you are running more processes than CPU cores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obcWCeYfBl-i"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwAgjuhBBmX4",
    "outputId": "c419e4f6-5064-4803-85ad-64a404851a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is 29321.3\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Writing the corrected MPI code to a file\n",
    "mpi_code = \"\"\"\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    int rank, p, i, root = 0;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &p);\n",
    "\n",
    "    // Make the local vector size constant\n",
    "    int local_vector_size = 100;\n",
    "\n",
    "    // Compute the global vector size\n",
    "    int n = p * local_vector_size;\n",
    "\n",
    "    // Initialize the vectors\n",
    "    double *a, *b;\n",
    "    a = (double *) malloc(local_vector_size * sizeof(double));\n",
    "    b = (double *) malloc(local_vector_size * sizeof(double));\n",
    "    for (i = 0; i < local_vector_size; i++) {\n",
    "        a[i] = 3.14 * rank;\n",
    "        b[i] = 6.67 * rank;\n",
    "    }\n",
    "\n",
    "    // Compute the local dot product\n",
    "    double partial_sum = 0.0;\n",
    "    for (i = 0; i < local_vector_size; i++) {\n",
    "        partial_sum += a[i] * b[i];\n",
    "    }\n",
    "\n",
    "    double sum = 0;\n",
    "    MPI_Reduce(&partial_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);\n",
    "\n",
    "    if (rank == root) {\n",
    "        // Corrected printf statement\n",
    "        printf(\"The dot product is %g\\\\n\", sum);\n",
    "    }\n",
    "\n",
    "    free(a);\n",
    "    free(b);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Save the corrected MPI code to a file\n",
    "with open('mpi_dot_product.c', 'w') as f:\n",
    "    f.write(mpi_code)\n",
    "\n",
    "# Step 3: Compile the MPI program using mpicc\n",
    "!mpicc -o mpi_dot_product mpi_dot_product.c\n",
    "\n",
    "# Step 4: Run the compiled MPI program with 4 processes and oversubscription\n",
    "!mpirun --oversubscribe -np 4 ./mpi_dot_product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4n8lYcNDpJs"
   },
   "source": [
    "# Exercise: Modify the MPI Dot Product Program\n",
    "\n",
    "In this exercise, you will modify the existing MPI program to better understand how the `MPI_Reduce` function works and how to handle larger vectors efficiently.\n",
    "\n",
    "### Task 1: Experiment with Larger Vector Sizes\n",
    "Currently, the local vector size is set to 100 for each process. Modify the program so that:\n",
    "1. The local vector size is increased to **1000** elements for each process.\n",
    "2. Observe the effect this change has on the **dot product** calculation. Does the result scale as expected?\n",
    "\n",
    "### Task 2: Use Different Operations in `MPI_Reduce`\n",
    "Currently, `MPI_Reduce` is used to sum the partial dot products from each process. Modify the program so that:\n",
    "1. Instead of summing, you use **`MPI_MAX`** to find the maximum dot product contribution from the processes.\n",
    "2. Print the result using the `MPI_MAX` operation to see how the values from different processes contribute to the final result.\n",
    "\n",
    "### Hints:\n",
    "- You can change the **reduce operation** in `MPI_Reduce` by replacing `MPI_SUM` with `MPI_MAX`.\n",
    "- Use larger vectors to understand the impact of data size on performance.\n",
    "- Make sure to print and compare the results with both **sum** and **maximum** reductions.\n",
    "\n",
    "After completing the tasks, run the program with different vector sizes and observe how the results and performance change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvaMaf90bb8k"
   },
   "source": [
    "## Advanced MPI Example: Point-to-Point vs Collective Operations\n",
    "\n",
    "In this section, we will explore a more complex MPI example that illustrates the difference between point-to-point and collective operations. The program will use both types of MPI communication to demonstrate how they work and when each is appropriate.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "The program performs the following tasks:\n",
    "1. **Initialization**:\n",
    "   - As in the previous example, the program starts by initializing the MPI environment and determining the rank and size of the processes.\n",
    "\n",
    "2. **Data Distribution Using Point-to-Point Communication**:\n",
    "   - Each process sends data to the next process in a ring-like fashion using `MPI_Send` and `MPI_Recv`.\n",
    "   - This operation mimics a manual data distribution where each process explicitly sends and receives data to and from its neighbors.\n",
    "\n",
    "3. **Data Collection Using Collective Communication**:\n",
    "   - All processes send their data to a root process using `MPI_Gather`, a collective operation that collects data from all processes and assembles it in the root process.\n",
    "\n",
    "4. **Broadcasting Data Using Collective Communication**:\n",
    "   - The root process broadcasts data to all other processes using `MPI_Bcast`, another collective operation that efficiently distributes data from one process to all others.\n",
    "\n",
    "5. **Finalization**:\n",
    "   - The program concludes by finalizing the MPI environment.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "1. **Initialization**:\n",
    "   - `MPI_Init(NULL, NULL);`: Initializes the MPI environment.\n",
    "   - `MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);`: Retrieves the rank (ID) of the calling process.\n",
    "   - `MPI_Comm_size(MPI_COMM_WORLD, &world_size);`: Retrieves the total number of processes.\n",
    "\n",
    "2. **Point-to-Point Communication**:\n",
    "   - **Sending Data**: Each process sends data to its neighbor using `MPI_Send`. For example, process 0 sends data to process 1, process 1 sends data to process 2, and so on. The last process sends data back to process 0, forming a ring.\n",
    "   - **Receiving Data**: Simultaneously, each process receives data from its neighbor using `MPI_Recv`.\n",
    "   - This operation is highly manual, as each process must explicitly specify the sender and receiver.\n",
    "\n",
    "3. **Collective Communication - Gathering Data**:\n",
    "   - **MPI_Gather**: This operation is used to collect data from all processes and store it in a single root process. Each process sends its data to the root, where it is gathered into a single array or list.\n",
    "   - Unlike point-to-point communication, `MPI_Gather` simplifies the process by automatically handling the collection of data from all processes.\n",
    "\n",
    "4. **Collective Communication - Broadcasting Data**:\n",
    "   - **MPI_Bcast**: This operation broadcasts data from the root process to all other processes. It is an efficient way to distribute the same data to all processes in the communicator.\n",
    "   - The root process sends its data once, and `MPI_Bcast` ensures that all processes receive it.\n",
    "\n",
    "5. **Finalization**:\n",
    "   - `MPI_Finalize();`: Cleans up the MPI environment.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Point-to-Point Communication**:\n",
    "  - `MPI_Send` and `MPI_Recv` are used for direct communication between two processes.\n",
    "  - This method is flexible but requires explicit management of senders and receivers, which can become complex in larger programs.\n",
    "\n",
    "- **Collective Communication**:\n",
    "  - `MPI_Gather` and `MPI_Bcast` are collective operations that involve all processes in the communicator.\n",
    "  - Collective operations are generally easier to use for common communication patterns, such as gathering data from all processes or broadcasting data to all processes.\n",
    "  - Collective operations are often more efficient than equivalent point-to-point operations, especially on large numbers of processes.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "Running this program with four processes might produce output similar to the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KErOIf_NbdBc",
    "outputId": "0aa11de1-3ca6-4ec0-ab7c-537b09089552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 3 sent data 103 to process 0 and received data 102 from process 2\n",
      "Process 0 sent data 100 to process 1 and received data 103 from process 3\n",
      "Process 1 sent data 101 to process 2 and received data 100 from process 0\n",
      "Process 2 sent data 102 to process 3 and received data 101 from process 1\n",
      "Root process 0 gathered data: 100 101 102 103 \n",
      "Process 0 received broadcast data: 500\n",
      "Process 2 received broadcast data: 500\n",
      "Process 1 received broadcast data: 500\n",
      "Process 3 received broadcast data: 500\n"
     ]
    }
   ],
   "source": [
    "# Save the MPI C code to a file\n",
    "mpi_code = \"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(NULL, NULL);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int world_size;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "\n",
    "    // Allocate some space for data\n",
    "    int data = 100 + world_rank;  // Unique data for each process\n",
    "\n",
    "    // Point-to-Point Communication: Ring Data Exchange\n",
    "    int next_rank = (world_rank + 1) % world_size;\n",
    "    int prev_rank = (world_rank - 1 + world_size) % world_size;\n",
    "    int received_data;\n",
    "\n",
    "    // Send data to the next process and receive data from the previous process\n",
    "    MPI_Send(&data, 1, MPI_INT, next_rank, 0, MPI_COMM_WORLD);\n",
    "    MPI_Recv(&received_data, 1, MPI_INT, prev_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "\n",
    "    printf(\"Process %d sent data %d to process %d and received data %d from process %d\\\\n\",\n",
    "            world_rank, data, next_rank, received_data, prev_rank);\n",
    "\n",
    "    // Collective Communication: Gather data at root\n",
    "    int* gathered_data = NULL;\n",
    "    if (world_rank == 0) {\n",
    "        gathered_data = (int*)malloc(sizeof(int) * world_size);\n",
    "    }\n",
    "    MPI_Gather(&data, 1, MPI_INT, gathered_data, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        printf(\"Root process %d gathered data: \", world_rank);\n",
    "        for (int i = 0; i < world_size; i++) {\n",
    "            printf(\"%d \", gathered_data[i]);\n",
    "        }\n",
    "        printf(\"\\\\n\");\n",
    "        free(gathered_data);\n",
    "    }\n",
    "\n",
    "    // Collective Communication: Broadcast data from root to all processes\n",
    "    int broadcast_data = 500;\n",
    "    if (world_rank == 0) {\n",
    "        broadcast_data = 500;  // Root sets the data to be broadcasted\n",
    "    }\n",
    "    MPI_Bcast(&broadcast_data, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    printf(\"Process %d received broadcast data: %d\\\\n\", world_rank, broadcast_data);\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Write the MPI code to a file\n",
    "with open('mpi_example.c', 'w') as f:\n",
    "    f.write(mpi_code)\n",
    "\n",
    "# Compile the MPI C code\n",
    "!mpicc -o mpi_example mpi_example.c\n",
    "\n",
    "# Run the compiled MPI program with 4 processes\n",
    "!mpirun --oversubscribe -np 4 ./mpi_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIwEzyYucX7W"
   },
   "source": [
    "## Explanation of the MPI Program Output\n",
    "\n",
    "The output of the MPI program provides insight into how the data was communicated between processes using both point-to-point and collective operations. Let's break down the key parts of the output.\n",
    "\n",
    "### Point-to-Point Communication (Ring Data Exchange)\n",
    "\n",
    "Each process sends its data to the next process in a circular manner (ring topology) and receives data from the previous process:\n",
    "\n",
    "- **Process 3 sent data 103 to process 0 and received data 102 from process 2**\n",
    "  - Process 3 sends its data (103) to process 0.\n",
    "  - Simultaneously, it receives data (102) from process 2.\n",
    "  \n",
    "- **Process 0 sent data 100 to process 1 and received data 103 from process 3**\n",
    "  - Process 0 sends its data (100) to process 1.\n",
    "  - Simultaneously, it receives data (103) from process 3.\n",
    "  \n",
    "- **Process 1 sent data 101 to process 2 and received data 100 from process 0**\n",
    "  - Process 1 sends its data (101) to process 2.\n",
    "  - Simultaneously, it receives data (100) from process 0.\n",
    "  \n",
    "- **Process 2 sent data 102 to process 3 and received data 101 from process 1**\n",
    "  - Process 2 sends its data (102) to process 3.\n",
    "  - Simultaneously, it receives data (101) from process 1.\n",
    "\n",
    "This part of the output shows that each process successfully communicated with its neighbors in the ring. The data exchange is point-to-point, meaning each process explicitly sends and receives data from specific processes.\n",
    "\n",
    "### Collective Communication - Gathering Data\n",
    "\n",
    "After the point-to-point communication, the program uses a collective operation, `MPI_Gather`, to collect data from all processes at the root process (process 0):\n",
    "\n",
    "- **Root process 0 gathered data: 100 101 102 103**\n",
    "  - The root process (process 0) gathers data from all processes in the communicator.\n",
    "  - The gathered data consists of the data from each process: 100 from process 0, 101 from process 1, 102 from process 2, and 103 from process 3.\n",
    "  \n",
    "This output confirms that the `MPI_Gather` operation successfully collected data from all processes into the root process.\n",
    "\n",
    "### Collective Communication - Broadcasting Data\n",
    "\n",
    "Finally, the program uses another collective operation, `MPI_Bcast`, to broadcast data from the root process (process 0) to all other processes:\n",
    "\n",
    "- **Process 0 received broadcast data: 500**\n",
    "- **Process 2 received broadcast data: 500**\n",
    "- **Process 1 received broadcast data: 500**\n",
    "- **Process 3 received broadcast data: 500**\n",
    "\n",
    "Here, the data value `500` is broadcasted by the root process (process 0) to all other processes. Each process receives this data and prints it, confirming that the broadcast was successful.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Point-to-Point Communication**: The data exchange between processes in a ring topology demonstrates how processes can communicate directly with each other using `MPI_Send` and `MPI_Recv`.\n",
    "- **Collective Communication - Gathering**: The `MPI_Gather` operation collects data from all processes and assembles it in the root process.\n",
    "- **Collective Communication - Broadcasting**: The `MPI_Bcast` operation efficiently distributes data from one process (the root) to all other processes.\n",
    "\n",
    "This output provides a clear example of both point-to-point and collective communication in an MPI program, showcasing how data can be exchanged and distributed among processes in a parallel computing environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQqWY3MHcYy0"
   },
   "outputs": [],
   "source": [
    "# MPI Collective Operations: A Hands-On Example\n",
    "\n",
    "In this section, we will explore some of the most popular MPI collective operations, which allow processes to communicate in different patterns. We will use a small vector and perform the following collective operations:\n",
    "- **MPI_Bcast**: Broadcasts data from one process (root) to all other processes.\n",
    "- **MPI_Scatter**: Divides the data into chunks and distributes them across multiple processes.\n",
    "- **MPI_Gather**: Gathers data from all processes and combines it at the root process.\n",
    "- **MPI_Reduce**: Reduces values from all processes (e.g., summing them) and stores the result at the root process.\n",
    "\n",
    "### Key Collective Operations:\n",
    "1. **MPI_Bcast**:\n",
    "   - The root process broadcasts a vector to all other processes.\n",
    "   - All processes receive the same vector from the root.\n",
    "\n",
    "2. **MPI_Scatter**:\n",
    "   - A vector is divided into equal parts, and each process receives one part (a chunk).\n",
    "\n",
    "3. **MPI_Gather**:\n",
    "   - Each process contributes a small vector (chunk), and the root process gathers these chunks to form the original vector.\n",
    "\n",
    "4. **MPI_Reduce**:\n",
    "   - Each process computes a local sum, and the root process reduces these sums (e.g., summing them all) to compute the global sum.\n",
    "\n",
    "### Example:\n",
    "We will use a small vector of 8 elements and visualize how each operation modifies the data across 4 processes. The root process will print the result for each operation, allowing you to see the differences between broadcasting, scattering, gathering, and reducing.\n",
    "\n",
    "### Task:\n",
    "Run the provided code with 4 processes and observe the data exchanges in each collective operation.\n",
    "\n",
    "Let's now compile and run the code using MPI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c4hjGEYC0SL",
    "outputId": "5b824f2f-2a69-43e1-91b0-6459a9171c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root process initial vector: 1 2 3 4 5 6 7 8 \n",
      "Process 0 received broadcasted vector: 2013448640 32767 2013448920 32767 1649308361 23581 1649308711 23581 \n",
      "Process 1 received broadcasted vector: 2013448640 32767 2013448920 32767 1649308361 23581 1649308711 23581 \n",
      "Process 0 received scattered vector: 1 2 \n",
      "Process 2 received broadcasted vector: 2013448640 32767 2013448920 32767 1649308361 23581 1649308711 23581 \n",
      "Process 2 received scattered vector: 5 6 \n",
      "Process 1 received scattered vector: 3 4 \n",
      "Process 3 received broadcasted vector: 2013448640 32767 2013448920 32767 1649308361 23581 1649308711 23581 \n",
      "Process 3 received scattered vector: 7 8 \n",
      "Root process gathered vector: 1 2 3 4 5 6 7 8 \n",
      "Global sum after reduction: 36\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Writing the MPI code to a file\n",
    "mpi_code = \"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    int root = 0;\n",
    "    int vector_size = 8;\n",
    "    int local_vector_size = vector_size / size;\n",
    "    int i;\n",
    "\n",
    "    // Define a vector to be used in the operations\n",
    "    int *vector = NULL;\n",
    "    if (rank == root) {\n",
    "        vector = (int *)malloc(vector_size * sizeof(int));\n",
    "        for (i = 0; i < vector_size; i++) {\n",
    "            vector[i] = i + 1;\n",
    "        }\n",
    "        printf(\"Root process initial vector: \");\n",
    "        for (i = 0; i < vector_size; i++) {\n",
    "            printf(\"%d \", vector[i]);\n",
    "        }\n",
    "        printf(\"\\\\n\");\n",
    "    }\n",
    "\n",
    "    // Broadcast: Send the vector from root to all processes\n",
    "    int recv_vector_bcast[vector_size];\n",
    "    MPI_Bcast(recv_vector_bcast, vector_size, MPI_INT, root, MPI_COMM_WORLD);\n",
    "    printf(\"Process %d received broadcasted vector: \", rank);\n",
    "    for (i = 0; i < vector_size; i++) {\n",
    "        printf(\"%d \", recv_vector_bcast[i]);\n",
    "    }\n",
    "    printf(\"\\\\n\");\n",
    "\n",
    "    // Scatter: Send chunks of the vector to each process\n",
    "    int local_vector[local_vector_size];\n",
    "    MPI_Scatter(vector, local_vector_size, MPI_INT, local_vector, local_vector_size, MPI_INT, root, MPI_COMM_WORLD);\n",
    "    printf(\"Process %d received scattered vector: \", rank);\n",
    "    for (i = 0; i < local_vector_size; i++) {\n",
    "        printf(\"%d \", local_vector[i]);\n",
    "    }\n",
    "    printf(\"\\\\n\");\n",
    "\n",
    "    // Gather: Collect local vectors from all processes to the root process\n",
    "    int *gathered_vector = NULL;\n",
    "    if (rank == root) {\n",
    "        gathered_vector = (int *)malloc(vector_size * sizeof(int));\n",
    "    }\n",
    "    MPI_Gather(local_vector, local_vector_size, MPI_INT, gathered_vector, local_vector_size, MPI_INT, root, MPI_COMM_WORLD);\n",
    "    if (rank == root) {\n",
    "        printf(\"Root process gathered vector: \");\n",
    "        for (i = 0; i < vector_size; i++) {\n",
    "            printf(\"%d \", gathered_vector[i]);\n",
    "        }\n",
    "        printf(\"\\\\n\");\n",
    "    }\n",
    "\n",
    "    // Reduce: Compute the sum of the local vectors and reduce at root\n",
    "    int local_sum = 0;\n",
    "    for (i = 0; i < local_vector_size; i++) {\n",
    "        local_sum += local_vector[i];\n",
    "    }\n",
    "    int global_sum = 0;\n",
    "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n",
    "    if (rank == root) {\n",
    "        printf(\"Global sum after reduction: %d\\\\n\", global_sum);\n",
    "    }\n",
    "\n",
    "    // Clean up\n",
    "    if (rank == root) {\n",
    "        free(vector);\n",
    "        free(gathered_vector);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Save the MPI code to a file\n",
    "with open('mpi_collective_operations.c', 'w') as f:\n",
    "    f.write(mpi_code)\n",
    "\n",
    "# Step 3: Compile the MPI program using mpicc\n",
    "!mpicc -o mpi_collective_operations mpi_collective_operations.c\n",
    "\n",
    "# Step 4: Run the compiled MPI program with 4 processes and oversubscription\n",
    "!mpirun --oversubscribe -np 4 ./mpi_collective_operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8Ze-hBiDdts"
   },
   "source": [
    "# Exercise: Modify the MPI Collective Operations Example\n",
    "\n",
    "In this exercise, you will modify the existing MPI program to better understand how different collective communication operations work in MPI.\n",
    "\n",
    "### Task: Modify the Vector Size and Experiment with Collective Operations\n",
    "The current code works with a vector of size 8, which is evenly distributed across 4 processes. Modify the program so that:\n",
    "1. The vector size is **not evenly divisible** by the number of processes (e.g., change the vector size to 10).\n",
    "2. Update the **MPI_Scatter** and **MPI_Gather** operations to handle this uneven distribution properly. This will require adjusting how chunks of the vector are scattered and gathered.\n",
    "\n",
    "### Hints:\n",
    "- You can use **`MPI_Scatterv`** and **`MPI_Gatherv`** to handle uneven distributions by specifying the size of each chunk explicitly.\n",
    "- The **root process** will still initialize the full vector, and the gathered result should be displayed correctly at the end.\n",
    "\n",
    "After making these changes, run the program and observe how the collective operations work with a vector size that isn't divisible by the number of processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Nonblocking Ring with Overlap (MPI_Isend/Irecv + compute)\n",
    "\n",
    "Each rank sends its value to `(rank+1) mod P` and receives from `(rank-1+P) mod P`.\n",
    "We use `MPI_Isend/MPI_Irecv` so ranks can **do computation while messages fly**, then `MPI_Waitall`.\n",
    "This illustrates nonblocking point-to-point comms from the slides. :contentReference[oaicite:1]{index=1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 3 received 1002 from rank 2\n",
      "Rank 1 received 1000 from rank 0\n",
      "Rank 0 received 1003 from rank 3\n",
      "Rank 2 received 1001 from rank 1\n"
     ]
    }
   ],
   "source": [
    "ring_nb = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "static void fake_compute(int iters) {\n",
    "  volatile double x = 0;\n",
    "  for (int i=0;i<iters;i++) x += i*1e-9;\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  int left  = (r - 1 + p) % p;\n",
    "  int right = (r + 1) % p;\n",
    "\n",
    "  int send_val = 1000 + r;\n",
    "  int recv_val = -1;\n",
    "\n",
    "  MPI_Request reqs[2];\n",
    "  MPI_Irecv(&recv_val, 1, MPI_INT, left,  0, MPI_COMM_WORLD, &reqs[0]);\n",
    "  MPI_Isend(&send_val, 1, MPI_INT, right, 0, MPI_COMM_WORLD, &reqs[1]);\n",
    "\n",
    "  // overlap some arbitrary work while comms progress\n",
    "  fake_compute(50*1000*100); // adjust to taste\n",
    "\n",
    "  MPI_Waitall(2, reqs, MPI_STATUSES_IGNORE);\n",
    "  printf(\"Rank %d received %d from rank %d\\n\", r, recv_val, left);\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"ring_nb.c\",\"w\").write(ring_nb)\n",
    "!mpicc -O2 ring_nb.c -o ring_nb\n",
    "!mpirun --oversubscribe -np 4 ./ring_nb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 2-D Cartesian Communicator and Neighbors\n",
    "\n",
    "Use `MPI_Dims_create` + `MPI_Cart_create` to arrange ranks in a 2-D grid and\n",
    "discover 4-neighbors with `MPI_Cart_shift`. This mirrors the slides’ discussion of\n",
    "communicators and rank topology and leads naturally into stencil codes. :contentReference[oaicite:2]{index=2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 -> cartRank 0 coords=(0,0) up=-2 down=2 left=-2 right=1\n",
      "Rank 1 -> cartRank 1 coords=(0,1) up=-2 down=3 left=0 right=-2\n",
      "Rank 2 -> cartRank 2 coords=(1,0) up=0 down=-2 left=-2 right=3\n",
      "Rank 3 -> cartRank 3 coords=(1,1) up=1 down=-2 left=2 right=-2\n"
     ]
    }
   ],
   "source": [
    "cart_code = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  int dims[2] = {0,0};\n",
    "  MPI_Dims_create(p, 2, dims); // choose a good 2D factorization\n",
    "  int periods[2] = {0,0};      // no wrap-around\n",
    "  MPI_Comm cart;\n",
    "  MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &cart);\n",
    "\n",
    "  int cr; MPI_Comm_rank(cart,&cr);\n",
    "  int coords[2]; MPI_Cart_coords(cart, cr, 2, coords);\n",
    "\n",
    "  int up, down, left, right;\n",
    "  MPI_Cart_shift(cart, 0, 1, &up, &down);   // vertical neighbors\n",
    "  MPI_Cart_shift(cart, 1, 1, &left, &right);// horizontal neighbors\n",
    "\n",
    "  printf(\"Rank %d -> cartRank %d coords=(%d,%d) up=%d down=%d left=%d right=%d\\n\",\n",
    "         r, cr, coords[0], coords[1], up, down, left, right);\n",
    "\n",
    "  MPI_Comm_free(&cart);\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"cart2d.c\",\"w\").write(cart_code)\n",
    "!mpicc -O2 cart2d.c -o cart2d\n",
    "!mpirun --oversubscribe -np 4 ./cart2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Uneven Data with MPI_Scatterv and MPI_Gatherv\n",
    "\n",
    "Real problems rarely split evenly. Here the root scatters N elements with ragged\n",
    "chunk sizes (`counts`, `displs`) and gathers them back. This extends the collectives\n",
    "you’ve already used and echoes the slides’ coverage of communication collectives. :contentReference[oaicite:3]{index=3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sum=483 (check)\n"
     ]
    }
   ],
   "source": [
    "scatterv_code = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(int argc,char**argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  int N = 23; // not divisible by p\n",
    "  int *A = NULL, *counts=NULL, *displs=NULL;\n",
    "\n",
    "  if (r==0){\n",
    "    A = (int*)malloc(N*sizeof(int));\n",
    "    for (int i=0;i<N;i++) A[i] = 10+i;\n",
    "    counts = (int*)malloc(p*sizeof(int));\n",
    "    displs = (int*)malloc(p*sizeof(int));\n",
    "    int base = N/p, rem = N%p, offset=0;\n",
    "    for (int k=0;k<p;k++){\n",
    "      counts[k] = base + (k<rem ? 1:0);\n",
    "      displs[k] = offset;\n",
    "      offset   += counts[k];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  int mycount;\n",
    "  MPI_Scatter(counts, 1, MPI_INT, &mycount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  int *chunk = (int*)malloc(mycount*sizeof(int));\n",
    "  MPI_Scatterv(A, counts, displs, MPI_INT, chunk, mycount, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  // do something local (sum)\n",
    "  int localsum=0; for (int i=0;i<mycount;i++) localsum += chunk[i];\n",
    "\n",
    "  // gather results (ragged)\n",
    "  int *sums = NULL;\n",
    "  if (r==0) sums = (int*)malloc(p*sizeof(int));\n",
    "  MPI_Gather(&localsum,1,MPI_INT, sums,1,MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  if (r==0){\n",
    "    int total=0; for(int k=0;k<p;k++) total += sums[k];\n",
    "    printf(\"Global sum=%d (check)\\n\", total);\n",
    "  }\n",
    "\n",
    "  free(chunk);\n",
    "  if (r==0){ free(A); free(counts); free(displs); free(sums); }\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"scatterv_gatherv.c\",\"w\").write(scatterv_code)\n",
    "!mpicc -O2 scatterv_gatherv.c -o scatterv_gatherv\n",
    "!mpirun --oversubscribe -np 4 ./scatterv_gatherv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Halo Exchange Columns via MPI_Type_vector (Derived Datatype)\n",
    "\n",
    "For 2-D grids, vertical halo columns aren’t contiguous. `MPI_Type_vector` lets you\n",
    "send a column as a single message (count = rows, blocklength = 1, stride = row pitch).\n",
    "This ties directly to the “User-Defined Data Types” section of the slides and sets up\n",
    "stencil mini-apps. :contentReference[oaicite:4]{index=4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 3 ghosts: top[1]=1 bottom[1]=0 left[1]=2 right[1]=0\n",
      "Rank 0 ghosts: top[1]=0 bottom[1]=2 left[1]=0 right[1]=1\n",
      "Rank 1 ghosts: top[1]=0 bottom[1]=3 left[1]=0 right[1]=0\n",
      "Rank 2 ghosts: top[1]=0 bottom[1]=0 left[1]=0 right[1]=3\n"
     ]
    }
   ],
   "source": [
    "halo_dt = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define IDX(i,j,ld) ((i)*(ld) + (j))\n",
    "\n",
    "int main(int argc,char**argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  // Build a 2D grid of ranks\n",
    "  int dims[2]={0,0}; MPI_Dims_create(p,2,dims);\n",
    "  int periods[2]={0,0}; MPI_Comm cart; MPI_Cart_create(MPI_COMM_WORLD,2,dims,periods,1,&cart);\n",
    "\n",
    "  int up,down,left,right;\n",
    "  MPI_Cart_shift(cart,0,1,&up,&down);\n",
    "  MPI_Cart_shift(cart,1,1,&left,&right);\n",
    "\n",
    "  // Local interior size (small for demo)\n",
    "  int nx = 8, ny = 6;             // interior cells\n",
    "  int ld = nx+2;                   // +2 for ghost columns (L/R)\n",
    "  double* A = (double*)calloc((ny+2)*ld, sizeof(double)); // +2 ghost rows (T/B)\n",
    "\n",
    "  // Fill interior with rank id\n",
    "  for(int i=1;i<=ny;i++) for(int j=1;j<=nx;j++) A[IDX(i,j,ld)] = (double)r;\n",
    "\n",
    "  // Derived datatype: one vertical column of ny elements with stride=ld\n",
    "  MPI_Datatype COL; MPI_Type_vector(ny, 1, ld, MPI_DOUBLE, &COL); MPI_Type_commit(&COL);\n",
    "\n",
    "  MPI_Request reqs[8]; int q=0;\n",
    "  // Post receives (T,B contiguous; L,R derived)\n",
    "  if (up   != MPI_PROC_NULL)   MPI_Irecv(&A[IDX(0,      1,ld)], nx, MPI_DOUBLE, up,   0, cart, &reqs[q++]); // top row\n",
    "  if (down != MPI_PROC_NULL)   MPI_Irecv(&A[IDX(ny+1,   1,ld)], nx, MPI_DOUBLE, down, 1, cart, &reqs[q++]); // bottom row\n",
    "  if (left != MPI_PROC_NULL)   MPI_Irecv(&A[IDX(1,      0,ld)], 1,  COL,         left, 2, cart, &reqs[q++]); // left col\n",
    "  if (right!= MPI_PROC_NULL)   MPI_Irecv(&A[IDX(1,  nx+1,ld)], 1,  COL,         right,3, cart, &reqs[q++]); // right col\n",
    "\n",
    "  // Sends\n",
    "  if (up   != MPI_PROC_NULL)   MPI_Isend(&A[IDX(1,      1,ld)], nx, MPI_DOUBLE, up,   1, cart, &reqs[q++]);\n",
    "  if (down != MPI_PROC_NULL)   MPI_Isend(&A[IDX(ny,     1,ld)], nx, MPI_DOUBLE, down, 0, cart, &reqs[q++]);\n",
    "  if (left != MPI_PROC_NULL)   MPI_Isend(&A[IDX(1,      1,ld)], 1,  COL,         left, 3, cart, &reqs[q++]);\n",
    "  if (right!= MPI_PROC_NULL)   MPI_Isend(&A[IDX(1,      nx,ld)], 1,  COL,         right,2, cart, &reqs[q++]);\n",
    "\n",
    "  MPI_Waitall(q, reqs, MPI_STATUSES_IGNORE);\n",
    "\n",
    "  // Sanity: print four ghost cells summary from each rank\n",
    "  printf(\"Rank %d ghosts: top[1]=%.0f bottom[1]=%.0f left[1]=%.0f right[1]=%.0f\\n\",\n",
    "         r, A[IDX(0,1,ld)], A[IDX(ny+1,1,ld)], A[IDX(1,0,ld)], A[IDX(1,nx+1,ld)]);\n",
    "\n",
    "  MPI_Type_free(&COL);\n",
    "  free(A);\n",
    "  MPI_Comm_free(&cart);\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"halo_datatype.c\",\"w\").write(halo_dt)\n",
    "!mpicc -O2 halo_datatype.c -o halo_datatype\n",
    "!mpirun --oversubscribe -np 4 ./halo_datatype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-App: Distributed 2-D Heat Diffusion (Jacobi) with Halo Exchange\n",
    "\n",
    "A classic HPC kernel. We split the global grid over a 2-D Cartesian communicator,\n",
    "do per-iteration halo exchanges (top/bottom contiguous, left/right via `MPI_Type_vector`),\n",
    "then update interior points. We use `MPI_Allreduce` for a global max-difference\n",
    "convergence check. This ties together communicators, collectives, nonblocking\n",
    "point-to-point, and derived datatypes from the slides. :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "Try:\n",
    "- change `Gx,Gy` (global grid), `ITERS`, and the process count (`-np`).\n",
    "- observe non-linear scaling (memory bandwidth & halos dominate at small sizes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi2D MPI: Gx=256 Gy=256 P=4 dims=2x2 ITERS=300 maxdiff=1.663e-05 time=0.081 s\n"
     ]
    }
   ],
   "source": [
    "jacobi_mpi = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define IDX(i,j,ld) ((i)*(ld) + (j))\n",
    "\n",
    "static void exchange_halos(double* A, int nx, int ny, int ld,\n",
    "                           MPI_Comm cart, int up,int down,int left,int right,\n",
    "                           MPI_Datatype COL)\n",
    "{\n",
    "  MPI_Request reqs[8]; int q=0;\n",
    "\n",
    "  // Post receives\n",
    "  if (up   != MPI_PROC_NULL)   MPI_Irecv(&A[IDX(0,    1,ld)], nx, MPI_DOUBLE, up,   0, cart, &reqs[q++]);\n",
    "  if (down != MPI_PROC_NULL)   MPI_Irecv(&A[IDX(ny+1, 1,ld)], nx, MPI_DOUBLE, down, 1, cart, &reqs[q++]);\n",
    "  if (left != MPI_PROC_NULL)   MPI_Irecv(&A[IDX(1,    0,ld)], 1,  COL,         left, 2, cart, &reqs[q++]);\n",
    "  if (right!= MPI_PROC_NULL)   MPI_Irecv(&A[IDX(1, nx+1,ld)], 1,  COL,         right,3, cart, &reqs[q++]);\n",
    "\n",
    "  // Sends\n",
    "  if (up   != MPI_PROC_NULL)   MPI_Isend(&A[IDX(1,    1,ld)], nx, MPI_DOUBLE, up,   1, cart, &reqs[q++] );\n",
    "  if (down != MPI_PROC_NULL)   MPI_Isend(&A[IDX(ny,   1,ld)], nx, MPI_DOUBLE, down, 0, cart, &reqs[q++] );\n",
    "  if (left != MPI_PROC_NULL)   MPI_Isend(&A[IDX(1,    1,ld)], 1,  COL,         left, 3, cart, &reqs[q++] );\n",
    "  if (right!= MPI_PROC_NULL)   MPI_Isend(&A[IDX(1,    nx,ld)], 1,  COL,         right,2, cart, &reqs[q++] );\n",
    "\n",
    "  MPI_Waitall(q, reqs, MPI_STATUSES_IGNORE);\n",
    "}\n",
    "\n",
    "int main(int argc,char**argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  // Global grid and iterations\n",
    "  int Gx = 256, Gy = 256, ITERS = 300;\n",
    "  if (argc>1) Gx = atoi(argv[1]);\n",
    "  if (argc>2) Gy = atoi(argv[2]);\n",
    "  if (argc>3) ITERS = atoi(argv[3]);\n",
    "\n",
    "  // Cartesian 2D grid of ranks\n",
    "  int dims[2]={0,0}; MPI_Dims_create(p,2,dims);\n",
    "  int periods[2]={0,0}; MPI_Comm cart; MPI_Cart_create(MPI_COMM_WORLD,2,dims,periods,1,&cart);\n",
    "\n",
    "  int coords[2]; int cr; MPI_Comm_rank(cart,&cr); MPI_Cart_coords(cart,cr,2,coords);\n",
    "  int up,down,left,right;\n",
    "  MPI_Cart_shift(cart,0,1,&up,&down);\n",
    "  MPI_Cart_shift(cart,1,1,&left,&right);\n",
    "\n",
    "  // Local interior sizes (block decomposition)\n",
    "  int nx = Gx / dims[1] + (coords[1] < (Gx % dims[1]) ? 1 : 0);\n",
    "  int ny = Gy / dims[0] + (coords[0] < (Gy % dims[0]) ? 1 : 0);\n",
    "\n",
    "  int ld = nx + 2; // +ghost cols\n",
    "  double *A = (double*)calloc((ny+2)*ld, sizeof(double));\n",
    "  double *B = (double*)calloc((ny+2)*ld, sizeof(double));\n",
    "\n",
    "  // Boundary conditions: set left global boundary to 1.0 (others 0.0)\n",
    "  // Determine if this rank owns the global left edge:\n",
    "  if (coords[1] == 0){\n",
    "    for(int i=1;i<=ny;i++) A[IDX(i,1,ld)] = 1.0;\n",
    "  }\n",
    "\n",
    "  // Derived datatype for a column halo\n",
    "  MPI_Datatype COL; MPI_Type_vector(ny, 1, ld, MPI_DOUBLE, &COL); MPI_Type_commit(&COL);\n",
    "\n",
    "  double t0 = MPI_Wtime();\n",
    "  double global_maxdiff = 0.0;\n",
    "\n",
    "  for (int it=0; it<ITERS; ++it){\n",
    "    exchange_halos(A, nx, ny, ld, cart, up,down,left,right, COL);\n",
    "\n",
    "    double local_maxdiff = 0.0;\n",
    "    for (int i=1;i<=ny;i++){\n",
    "      for (int j=1;j<=nx;j++){\n",
    "        B[IDX(i,j,ld)] = 0.25 * ( A[IDX(i-1,j,ld)] + A[IDX(i+1,j,ld)]\n",
    "                                + A[IDX(i,j-1,ld)] + A[IDX(i,j+1,ld)] );\n",
    "        double d = fabs(B[IDX(i,j,ld)] - A[IDX(i,j,ld)]);\n",
    "        if (d > local_maxdiff) local_maxdiff = d;\n",
    "      }\n",
    "    }\n",
    "\n",
    "    // swap\n",
    "    double *tmp=A; A=B; B=tmp;\n",
    "\n",
    "    // Convergence check across all ranks\n",
    "    MPI_Allreduce(&local_maxdiff, &global_maxdiff, 1, MPI_DOUBLE, MPI_MAX, cart);\n",
    "    if (global_maxdiff < 1e-6) break;\n",
    "  }\n",
    "\n",
    "  double t = MPI_Wtime() - t0;\n",
    "\n",
    "  if (r==0){\n",
    "    printf(\"Jacobi2D MPI: Gx=%d Gy=%d P=%d dims=%dx%d ITERS=%d maxdiff=%.3e time=%.3f s\\n\",\n",
    "           Gx,Gy,p,dims[0],dims[1],ITERS,global_maxdiff,t);\n",
    "  }\n",
    "\n",
    "  MPI_Type_free(&COL);\n",
    "  free(A); free(B);\n",
    "  MPI_Comm_free(&cart);\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"jacobi_mpi.c\",\"w\").write(jacobi_mpi)\n",
    "!mpicc -O3 jacobi_mpi.c -o jacobi_mpi\n",
    "# 2x2 process grid is a nice default:\n",
    "!mpirun --oversubscribe -np 4 ./jacobi_mpi 256 256 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-App: Dynamic Master/Worker Task Farm (MPI_ANY_SOURCE)\n",
    "\n",
    "Real workloads are imbalanced. The master (rank 0) hands out different-cost tasks\n",
    "to workers. Workers request new tasks as they finish. We rely on `MPI_Recv` with\n",
    "`MPI_ANY_SOURCE` to process results in the order they complete, not by rank, a pattern\n",
    "also shown conceptually in your slides’ manager/worker discussion. :contentReference[oaicite:6]{index=6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master: aggregated XOR(total)=49760 over 24 tasks\n"
     ]
    }
   ],
   "source": [
    "farm = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "enum { TAG_TASK=1, TAG_RESULT=2, TAG_DONE=3 };\n",
    "\n",
    "static long do_work(int n){\n",
    "  // variable-cost fake work\n",
    "  volatile double x=0; for (int i=0;i<n*20000;i++) x += i*1e-9;\n",
    "  return (long)(x) ^ n;\n",
    "}\n",
    "\n",
    "int main(int argc,char**argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  if (p < 2){ if(r==0) printf(\"Run with at least 2 processes\\n\"); MPI_Finalize(); return 0; }\n",
    "\n",
    "  if (r==0){\n",
    "    int num_tasks = 24;\n",
    "    int next_task = 0;\n",
    "    // prime the pump: send one task to each worker\n",
    "    for (int w=1; w<p && next_task<num_tasks; ++w){\n",
    "      int n = 100 + (next_task%8)*50; // variable cost\n",
    "      MPI_Send(&n,1,MPI_INT,w,TAG_TASK,MPI_COMM_WORLD);\n",
    "      next_task++;\n",
    "    }\n",
    "    int completed = 0;\n",
    "    long total = 0;\n",
    "    while (completed < num_tasks){\n",
    "      long result; MPI_Status st;\n",
    "      MPI_Recv(&result,1,MPI_LONG,MPI_ANY_SOURCE,TAG_RESULT,MPI_COMM_WORLD,&st);\n",
    "      total ^= result;\n",
    "      completed++;\n",
    "\n",
    "      if (next_task < num_tasks){\n",
    "        int n = 100 + (next_task%8)*50;\n",
    "        MPI_Send(&n,1,MPI_INT,st.MPI_SOURCE,TAG_TASK,MPI_COMM_WORLD);\n",
    "        next_task++;\n",
    "      } else {\n",
    "        int dummy=0;\n",
    "        MPI_Send(&dummy,1,MPI_INT,st.MPI_SOURCE,TAG_DONE,MPI_COMM_WORLD);\n",
    "      }\n",
    "    }\n",
    "    // tell any idle workers to stop\n",
    "    for (int w=1; w<p; ++w){\n",
    "      MPI_Send(&(int){0},1,MPI_INT,w,TAG_DONE,MPI_COMM_WORLD);\n",
    "    }\n",
    "    printf(\"Master: aggregated XOR(total)=%ld over %d tasks\\n\", total, num_tasks);\n",
    "  } else {\n",
    "    while (1){\n",
    "      MPI_Status st; int n;\n",
    "      MPI_Recv(&n,1,MPI_INT,0,MPI_ANY_TAG,MPI_COMM_WORLD,&st);\n",
    "      if (st.MPI_TAG == TAG_DONE) break;\n",
    "      long res = do_work(n);\n",
    "      MPI_Send(&res,1,MPI_LONG,0,TAG_RESULT,MPI_COMM_WORLD);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"task_farm.c\",\"w\").write(farm)\n",
    "!mpicc -O2 task_farm.c -o task_farm\n",
    "!mpirun --oversubscribe -np 6 ./task_farm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: MPI-IO (write one shared file)\n",
    "\n",
    "Each rank writes its chunk into a single file at a disjoint offset using `MPI_File_write_at`.\n",
    "Demonstrates parallel I/O—very common in production HPC. (Keep paths writable in Colab.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8 integers per rank to mpiio_output.bin\n",
      "-rw-r--r--. 1 user01 user01 128 Sep 14 20:46 mpiio_output.bin\n"
     ]
    }
   ],
   "source": [
    "mpiio = r\"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(int argc,char**argv){\n",
    "  MPI_Init(&argc,&argv);\n",
    "  int r,p; MPI_Comm_rank(MPI_COMM_WORLD,&r); MPI_Comm_size(MPI_COMM_WORLD,&p);\n",
    "\n",
    "  int nloc = 8;\n",
    "  int *buf = (int*)malloc(nloc*sizeof(int));\n",
    "  for (int i=0;i<nloc;i++) buf[i] = 100*r + i;\n",
    "\n",
    "  MPI_File fh;\n",
    "  MPI_File_open(MPI_COMM_WORLD, \"mpiio_output.bin\",\n",
    "                MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);\n",
    "\n",
    "  MPI_Offset off = (MPI_Offset)r * nloc * sizeof(int);\n",
    "  MPI_File_write_at(fh, off, buf, nloc, MPI_INT, MPI_STATUS_IGNORE);\n",
    "  MPI_File_close(&fh);\n",
    "\n",
    "  if (r==0) printf(\"Wrote %d integers per rank to mpiio_output.bin\\n\", nloc);\n",
    "  free(buf);\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "open(\"mpiio.c\",\"w\").write(mpiio)\n",
    "!mpicc -O2 mpiio.c -o mpiio\n",
    "!mpirun --oversubscribe -np 4 ./mpiio\n",
    "!ls -lh mpiio_output.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
